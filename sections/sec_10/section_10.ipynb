{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 10 Introduction to Machine Learning and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example I: Single-variable (1D) Linear Regression\n",
    "\n",
    "### **Problem**\n",
    "Given the *training dataset* $\\left(x^{(i)}\\in\\mathbb{R},y^{(i)}\\in\\mathbb{R}\\right)$, $i= 1,2,..., N$ (in other words the $x$-values and $y$-values are real numbers), we want to find the linear function $$y\\approx f(x)=wx +b$$ that fits the relations between $x^{(i)}$ and $y^{(i)}$. So that given any new $x^{test}$ in the **test** dataset, we can make the prediction $$y^{pred} = w x^{test}+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To compare to section 9's notation:\n",
    "\n",
    "$$ \\theta = (w,b) $$\n",
    "$$ y\\approx f(x; \\theta) = f(x; w, b)=wx +b$$ In the above line, the semicolon separates variables into two categories: data variable $x$ to the left, and function parameters $\\theta$ (or $w,b$) on the right. Our model depends on the parameters, but they won't change after we find the line of best fit.\n",
    "\n",
    "For the rest of section 10, we will be dropping the mention of $\\theta$ and it's associated variables in our notation for $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "- With the training dataset, define the loss function $L(w,b)$ of parameter $w$ and $b$, which is also called **mean squared error** (MSE) $$\\text{MSE} = L(w,b)=\\frac{1}{N}\\sum_{i=1}^N\\big(\\hat{y}^{(i)}-y^{(i)}\\big)^2=\\frac{1}{N}\\sum_{i=1}^N\\big((wx^{(i)}+b)-y^{(i)}\\big)^2,$$ where $\\hat{y}^{(i)}$ denotes the predicted value of y at $x^{(i)}$, i.e. $\\hat{y}^{(i)} = wx^{(i)}+b$.\n",
    "\n",
    "\n",
    "- Then find the minimum of the loss function. Note $L(w,b)$ is a quadratic function of $w$ and $b$, and we can analytically solve $\\partial_{w}L = \\partial_{b}L =0$ (exactly the same as finding the minimum in multivariable calculus -- Math 2D), and yields\n",
    "\n",
    "$$ w^* =\\frac{\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})(y^{(i)}-\\bar{y})}{\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})^{2}} = \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})(y^{(i)}-\\bar{y})}{\\frac{1}{N}\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})^{2}} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)},$$\n",
    "\n",
    "$$ b^* = \\bar{y}  - w^*\\bar{x},$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean of $x$ and of $y$, and $\\text{Cov}(X,Y)$ denotes the estimated covariance (or called sample covariance) between $X$ and $Y$ (a little difference with what you learn in statistics is that we have the normalization factor $1/N$ instead of $1/(N-1)$ here), and $\\text{Var}(Y)$ denotes the sample variance of $Y$ (the normalization factor is still $1/N$). This is just about convention -- in statistics, they pursue the unbiased estimator.)\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "- MSE: The smaller MSE indicates better performance\n",
    "- R-Squared: The larger $R^{2}$ (closer to 1) indicates better performance. Compared with MSE, R-squared is **dimensionless**, not dependent on the units of variable. \n",
    "\n",
    "$$R^2 = \\frac{\\text{Var}(Y) - \\text{MSE}}{\\text{Var}(Y)}$$\n",
    "\n",
    "The above calculation is a theoretical explanation for $R^2$. The equation below is for simpler calculation.\n",
    "\n",
    "$$R^{2} = 1 - \\frac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\text{MSE}}{\\text{Var}(Y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLinearRegression1D:\n",
    "    '''\n",
    "    The single-variable linear regression estimator -- writing in the style of sklearn package\n",
    "    '''\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        Determine the optimal parameters w, b for the input data x and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "           x : 1D numpy array with shape (n_samples,) from training data\n",
    "           y : 1D numpy array with shape (n_samples,) from training data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self, with new attributes slope w (float) and intercept b (float)\n",
    "         '''\n",
    "        \n",
    "        cov_mat = np.cov(x,y,bias=True) # covariance matrix, bias = True makes the factor is 1/N -- but it doesn't matter actually, since the factor will be cancelled\n",
    "        #array of form [[Var(x), Cov(x,y)],\n",
    "        #                [Cov(x,y), Var(y))]]\n",
    "        self.w = cov_mat[0,1] / cov_mat[0,0] # the (0,1) element is COV(X,Y) and (0,0) element is Var(X). (1,1) is Var(Y)\n",
    "        self.b =  np.mean(y)-self.w * np.mean(x)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        '''\n",
    "        Predict the output values for the input value x, based on trained parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "           x : 1D numpy array from training or test data \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        returns 1D numpy array of same shape as input, the predicted y value of corresponding x\n",
    "        '''\n",
    "        \n",
    "        return self.w*x+self.b #uses array vectorization to avoid loop. Entire array is scalar multiplied by w, and all terms +b.\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        '''\n",
    "        Calculate the R-squared on the dataset with input x and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "           x : 1D numpy array with shape (n_samples,) from training or test data\n",
    "           y : 1D numpy array with shape (n_samples,) from training or test data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        returns float, the R^2 value\n",
    "        '''\n",
    "        \n",
    "        y_hat = self.predict (x) # predicted y\n",
    "        mse = np.mean((y-y_hat)**2) # mean squared error\n",
    "        return 1- mse / np.var(y) # return R-squared\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>984200590</td>\n",
       "      <td>20150310T000000</td>\n",
       "      <td>315001.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>10230</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98058</td>\n",
       "      <td>47.4349</td>\n",
       "      <td>-122.168</td>\n",
       "      <td>1770</td>\n",
       "      <td>8374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19759</th>\n",
       "      <td>8924100370</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>1210000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3590</td>\n",
       "      <td>5335</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3140</td>\n",
       "      <td>450</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6762</td>\n",
       "      <td>-122.267</td>\n",
       "      <td>2100</td>\n",
       "      <td>6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10698</th>\n",
       "      <td>537000075</td>\n",
       "      <td>20140811T000000</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2300</td>\n",
       "      <td>8000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>870</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3293</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>2070</td>\n",
       "      <td>10200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9365</th>\n",
       "      <td>5708500315</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>615000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2130</td>\n",
       "      <td>4180</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1270</td>\n",
       "      <td>860</td>\n",
       "      <td>1926</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5750</td>\n",
       "      <td>-122.388</td>\n",
       "      <td>1710</td>\n",
       "      <td>4180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>7853250070</td>\n",
       "      <td>20150417T000000</td>\n",
       "      <td>679975.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3830</td>\n",
       "      <td>4644</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2900</td>\n",
       "      <td>930</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5384</td>\n",
       "      <td>-121.880</td>\n",
       "      <td>3400</td>\n",
       "      <td>6163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "4479    984200590  20150310T000000   315001.0         3       1.75   \n",
       "19759  8924100370  20140915T000000  1210000.0         4       3.50   \n",
       "10698   537000075  20140811T000000   420000.0         3       2.75   \n",
       "9365   5708500315  20140623T000000   615000.0         5       2.00   \n",
       "10108  7853250070  20150417T000000   679975.0         4       2.50   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "4479          1500     10230     1.0           0     0  ...      7   \n",
       "19759         3590      5335     2.0           0     2  ...      9   \n",
       "10698         2300      8000     1.0           0     0  ...      7   \n",
       "9365          2130      4180     1.5           0     0  ...      7   \n",
       "10108         3830      4644     2.0           0     0  ...      8   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "4479         1500              0      1968             0    98058  47.4349   \n",
       "19759        3140            450      2006             0    98115  47.6762   \n",
       "10698        1430            870      1965             0    98003  47.3293   \n",
       "9365         1270            860      1926             0    98116  47.5750   \n",
       "10108        2900            930      2004             0    98065  47.5384   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "4479  -122.168           1770        8374  \n",
       "19759 -122.267           2100        6250  \n",
       "10698 -122.306           2070       10200  \n",
       "9365  -122.388           1710        4180  \n",
       "10108 -121.880           3400        6163  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "house = pd.read_csv('kc_house_data.csv')\n",
    "house.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0      221900.0         3       1.00         1180      5650     1.0   \n",
       "1      538000.0         3       2.25         2570      7242     2.0   \n",
       "2      180000.0         2       1.00          770     10000     1.0   \n",
       "3      604000.0         4       3.00         1960      5000     1.0   \n",
       "4      510000.0         3       2.00         1680      8080     1.0   \n",
       "...         ...       ...        ...          ...       ...     ...   \n",
       "21608  360000.0         3       2.50         1530      1131     3.0   \n",
       "21609  400000.0         4       2.50         2310      5813     2.0   \n",
       "21610  402101.0         2       0.75         1020      1350     2.0   \n",
       "21611  400000.0         3       2.50         1600      2388     2.0   \n",
       "21612  325000.0         2       0.75         1020      1076     2.0   \n",
       "\n",
       "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "0               0     0          3      7        1180              0   \n",
       "1               0     0          3      7        2170            400   \n",
       "2               0     0          3      6         770              0   \n",
       "3               0     0          5      7        1050            910   \n",
       "4               0     0          3      8        1680              0   \n",
       "...           ...   ...        ...    ...         ...            ...   \n",
       "21608           0     0          3      8        1530              0   \n",
       "21609           0     0          3      8        2310              0   \n",
       "21610           0     0          3      7        1020              0   \n",
       "21611           0     0          3      8        1600              0   \n",
       "21612           0     0          3      7        1020              0   \n",
       "\n",
       "       sqft_living15  sqft_lot15  \n",
       "0               1340        5650  \n",
       "1               1690        7639  \n",
       "2               2720        8062  \n",
       "3               1360        5000  \n",
       "4               1800        7503  \n",
       "...              ...         ...  \n",
       "21608           1530        1509  \n",
       "21609           1830        7200  \n",
       "21610           1020        2007  \n",
       "21611           1410        1287  \n",
       "21612           1020        1357  \n",
       "\n",
       "[21613 rows x 14 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.drop(['id','date','zipcode','lat','long','yr_built','yr_renovated'],axis = 1, inplace = True)\n",
    "house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='X_house'></a>\n",
    "(This cell is a bookmark to the assignment of X below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = house.iloc[:,1:].to_numpy() #every column except price\n",
    "y = house['price'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21613, 13)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'price')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = X[:,2]\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('sqft_living') \n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MyLinearRegression1D in module __main__ object:\n",
      "\n",
      "class MyLinearRegression1D(builtins.object)\n",
      " |  The single-variable linear regression estimator -- writing in the style of sklearn package\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, x, y)\n",
      " |      Determine the optimal parameters w, b for the input data x and y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |         x : 1D numpy array with shape (n_samples,) from training data\n",
      " |         y : 1D numpy array with shape (n_samples,) from training data\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self, with new attributes slope w (float) and intercept b (float)\n",
      " |  \n",
      " |  predict(self, x)\n",
      " |      Predict the output values for the input value x, based on trained parameters\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |         x : 1D numpy array from training or test data \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      returns 1D numpy array of same shape as input, the predicted y value of corresponding x\n",
      " |  \n",
      " |  score(self, x, y)\n",
      " |      Calculate the R-squared on the dataset with input x and y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |         x : 1D numpy array with shape (n_samples,) from training or test data\n",
      " |         y : 1D numpy array with shape (n_samples,) from training or test data\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      returns float, the R^2 value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = MyLinearRegression1D() # initialize the instance of one estimator\n",
    "help(lreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49286538652201417"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19b68a8db80>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(dpi = 100)\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('sqft_living') \n",
    "plt.ylabel('price')\n",
    "plt.plot(x,lreg.predict(x),'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model # compare with the scikit learn package\n",
    "lreg_sklearn = linear_model.LinearRegression()\n",
    "lreg_sklearn.fit(x.reshape(-1,1),y) #only accept 2D-array as x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.8066899295007 -43867.60153385566\n",
      "[280.80668993] -43867.601533855544\n"
     ]
    }
   ],
   "source": [
    "print(lreg.w,lreg.b) #coeffs from our method\n",
    "print(lreg_sklearn.coef_, lreg_sklearn.intercept_) #coeffs from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49286538652201417"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_sklearn.score(x.reshape(-1,1),y) #same R^2 regression score up to 15 significant digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LinearRegression in module sklearn.linear_model._base object:\n",
      "\n",
      "class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      " |  LinearRegression(*, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)\n",
      " |  \n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      " |  to minimize the residual sum of squares between the observed targets in\n",
      " |  the dataset, and the targets predicted by the linear approximation.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to False, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `normalize` was deprecated in version 1.0 and will be\n",
      " |         removed in 1.2.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to use for the computation. This will only provide\n",
      " |      speedup in case of sufficiently large problems, that is if firstly\n",
      " |      `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
      " |      to `True`. ``None`` means 1 unless in a\n",
      " |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      " |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      When set to ``True``, forces the coefficients to be positive. This\n",
      " |      option is only supported for dense arrays.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  rank_ : int\n",
      " |      Rank of matrix `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  singular_ : array of shape (min(X, y),)\n",
      " |      Singular values of `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  intercept_ : float or array of shape (n_targets,)\n",
      " |      Independent term in the linear model. Set to 0.0 if\n",
      " |      `fit_intercept = False`.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  Ridge : Ridge regression addresses some of the\n",
      " |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      " |      size of the coefficients with l2 regularization.\n",
      " |  Lasso : The Lasso is a linear model that estimates\n",
      " |      sparse coefficients with l1 regularization.\n",
      " |  ElasticNet : Elastic-Net is a linear regression\n",
      " |      model trained with both l1 and l2 -norm regularization of the\n",
      " |      coefficients.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
      " |  (scipy.optimize.nnls) wrapped as a predictor object.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import LinearRegression\n",
      " |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      " |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      " |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      " |  >>> reg = LinearRegression().fit(X, y)\n",
      " |  >>> reg.score(X, y)\n",
      " |  1.0\n",
      " |  >>> reg.coef_\n",
      " |  array([1., 2.])\n",
      " |  >>> reg.intercept_\n",
      " |  3.0...\n",
      " |  >>> reg.predict(np.array([[3, 5]]))\n",
      " |  array([16.])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values. Will be cast to X's dtype if necessary.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted Estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lreg_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example II: Multi-variable Linear Regression (OLS -- Ordinary Least Square)\n",
    "\n",
    "### **Problem**\n",
    "Given the *training dataset* $\\left(x^{(i)},y^{(i)}\\right), i= 1,2,..., N$, this time with $y^{(i)}\\in \\mathbb{R}$ and $x^{(i)}\\in\\mathbb{R}^{p}$, we fit the multi-variable linear function\n",
    "\n",
    "$$y\\approx\\mathbf{f}(x)=\\beta_{0}+\\beta_{1}x_{1}+..+\\beta_{p}x_{p} = \\tilde{x}\\beta,$$  \n",
    "    $$\\tilde{x}=(1,x_{1},..,x_{p})\\in\\mathbb{R}^{1\\times (p+1)},\\beta = (\\beta_{0},\\beta_{1},..,\\beta_{p})^{T}\\in\\mathbb{R}^{(p+1)\\times 1}.$$\n",
    "\n",
    "\n",
    "We call $\\beta$ the regression coefficients, and $\\beta_{0}$ specially refers to the intercept.\n",
    "\n",
    "When $p = 2$, we are finding a plane of best fit. When $p > 2$, we are finding a hyperplane of best fit.\n",
    "\n",
    "Note that the dot product of $\\tilde{x}\\beta = f(x)$ (try this out).\n",
    "\n",
    "Using the whole training dataset, we can write as \n",
    "\n",
    "$$Y=\\left(\n",
    " \\begin{matrix}\n",
    "   y^{(1)}\\\\\n",
    "   y^{(2)} \\\\\n",
    "   \\cdots \\\\\n",
    "   y^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\approx\\left(\n",
    "  \\begin{matrix}\n",
    "   \\mathbf{f}(x^{(1)})\\\\\n",
    "   \\mathbf{f}(x^{(2)})\\\\\n",
    "   \\cdots \\\\\n",
    "   \\mathbf{f}(x^{(N)})\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\beta\\\\\n",
    "   \\tilde{x}^{(2)}\\beta\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\\beta\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\\\\n",
    "   \\tilde{x}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\beta = \\tilde{X}\\beta,\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\tilde{X}=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\\\\n",
    "   \\tilde{x}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right) =\\left(\n",
    "  \\begin{matrix}\n",
    "   1& x_{1}^{(1)} & \\cdots & x_{p}^{(1)}\\\\\n",
    "   1& x_{1}^{(2)} & \\cdots & x_{p}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   1& x_{1}^{(N)} & \\cdots & x_{p}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\n",
    "$$\n",
    "is also called the augmented data matrix.\n",
    "\n",
    "- **Question**: To get unknown $\\beta$, can we directly solve the linear equation $\\tilde{X}\\beta = Y$?\n",
    "\n",
    "- **Answer**: Most times no, because:\n",
    "    1. typically there are more equations than variables ($N>>(p+1)$) \n",
    "    2. the linear model is merely the approximation to the real mapping \n",
    "    3. there are noises in the data points -- it's highly possible that there is NO solution at all!\n",
    "\n",
    "- **Strategy**: Instead of solving $\\tilde{X}\\beta = Y$ exactly, we want $\\beta$ such that $\\tilde{X}\\beta$ is as close as $Y$ as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "- With the training dataset, define the loss function $L(\\beta)$ of parameters $\\beta$, which is also called **mean squared error** (MSE) $$\\text{MSE} = L(\\beta)=\\frac{1}{N}\\sum_{i=1}^N\\big(\\hat{y}^{(i)}-y^{(i)}\\big)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\left(\\tilde{x}^{(i)}\\beta - y^{(i)}\\right)^{2},$$\n",
    "where $\\hat{y}^{(i)}$ denotes the predicted value of y at $x^{(i)}$, i.e. $$\\hat{y}^{(i)} = \\beta_{0}+\\beta_{1}x^{(i)}_{1}+..+\\beta_{p}x^{(i)}_{p} = \\tilde{x}^{(i)}\\beta.$$ \n",
    "    \n",
    "    Now the problem becomes $$\\min_{\\beta}L(\\beta),$$ i.e. find the minimizer of a multi-variable (p+1 dimensions) function.\n",
    "\n",
    "\n",
    "- Then find the minimum of loss function -- There are two ways, either by numerical optimization (will be introduced in discussion) or by solving linear systems (introduced below), which is also called the **normal equation** approach.\n",
    "\n",
    "\n",
    "To solve the critical points, we have $\\nabla L(\\beta)=0$ (same as in multivariable calculus).\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\beta_{0}}&=2\\sum_{i=1}^{N}(\\tilde{x}^{(i)}\\beta-y^{(i)})=0,\\\\\n",
    "\\frac{\\partial L}{\\partial \\beta_{k}}&=2\\sum_{i=1}^{N} x_{k}^{(i)}(\\tilde{x}^{(i)}\\beta-y^{(i)})=0,\\quad k=1,2,..,p.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In Matrix form, it can be expressed as (left as exercise) $$\\tilde{X}^{T}\\tilde{X}\\beta=\\tilde{X}^{T}Y,$$\n",
    "\n",
    "also called the **normal equation** of linear regression. \n",
    "The optimal parameter \n",
    "$\\hat{\\beta}=\\text{argmin} L(\\beta)$\n",
    "is also called the ordinary least square (**OLS**) estimator in statistics community.\n",
    "\n",
    "Then the OLS estimator can be solved as $$\\hat{\\beta}=(\\tilde{X}^{T}\\tilde{X})^{-1}\\tilde{X}^{T}Y.$$\n",
    "\n",
    "**[Geometrical Interpretation](https://en.wikipedia.org/wiki/Ordinary_least_squares#Projection)**\n",
    "\n",
    "Denote $\\tilde{X}=(\\tilde{X}_{0},\\tilde{X}_{1},..,\\tilde{X}_{p})$, then $\\tilde{X}\\beta=\\sum_{k=0}^{p}\\beta_{k}\\tilde{X}_{k}$. We require that the residual $Y-\\tilde{X}\\beta$ is orthogonal (i.e. normal, or vertical) to the plane spanned by $\\tilde{X}_{k}$, which yields $$\\tilde{X}_{k}^{T}(Y-\\tilde{X}\\beta)=0,\\quad k = 0,1,...,p$$\n",
    "\n",
    "**Exercise**: Check that when $p=1$, the solution is equivalent to the single-variable regression. \n",
    "\n",
    "### Prediction in Test Data\n",
    "\n",
    "Given the new observation called $x^{(test)}$, we have the prediction as $$\\hat{y}^{(test)}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x^{(test)}_{1}+..+\\hat{\\beta}_{p}x^{(test)}_{p} = \\tilde{x}^{(test)}\\hat{\\beta}.$$\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "- MSE: The smaller MSE indicates better performance\n",
    "- R-Squared: The larger $R^{2}$ (closer to 1) indicates better performance. Compared with MSE, R-squared is **dimensionless**, not dependent on the units of variable. \n",
    "\n",
    "$$R^{2} = 1 - \\frac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\text{MSE}}{\\text{Var}(Y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coding of multi-variable linear regression left as the homework this week. Below we will call the function in sklearn directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to where X and y are defined in our notebook](#X_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6070919341230852"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model # compare with the scikit learn package\n",
    "lreg_sklearn = linear_model.LinearRegression()\n",
    "lreg_sklearn.fit(X,y) \n",
    "lreg_sklearn.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.46669591e+04, -1.48698203e+04,  1.34078969e+02,  2.67695148e-02,\n",
       "       -3.00709892e+03,  5.85782331e+05,  5.98910372e+04,  5.38601479e+04,\n",
       "        1.00892579e+05,  5.22508347e+01,  8.18281340e+01,  1.10527941e+01,\n",
       "       -7.49796930e-01])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_sklearn.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-690582.9505203539"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_sklearn.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivating Example III: Single-variable Polynomial Regression (Special Case of Multivariable Linear Regression)\n",
    "\n",
    "### **Problem**\n",
    "Given the *training dataset* $(x^{(i)},y^{(i)}), i= 1,2,..., N$, this time with $y^{(i)}\\in \\mathbb{R}$ and $x^{(i)}\\in\\mathbb{R}$, we fit the single-variable polynomial function of $p$-th order\n",
    "\n",
    "$$y\\approx f(x)=w_{0}+w_{1}x+w_{2}x^{2}+...+w_{p}x^{p}$$  \n",
    "\n",
    "**Remark:** A basic conclusion in numerical analysis is that with N points, we can have a polynomial of order (N-1) that fits every point perfectly.\n",
    " \n",
    "### **Strategy**\n",
    "Single-variable **polynomial regression** is a special case of multi-variable **linear** regression, because we can construct a dataset of $p$ variables by defining each row as $(1, x,x^{2}, ..., x^{p})$ for each observation at $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "___\n",
    "##  Machine Learning: Overview of the whole picture\n",
    "Possible hierarchies of machine learning concepts:\n",
    "\n",
    "- **Problems**: Supervised Learning (Regression,Classification), Unsupervised Learning (Dimension Reduction, Clustering), Reinforcement Learning (Not covered in this course)\n",
    "\n",
    "\n",
    "- **Models**: \n",
    "    - (Supervised) Linear Regression, Logistic Regression, K-Nearest Neighbor (kNN) Classification/Regression, Decision Tree, Random Forest, Support Vector Machine, Ensemble Method, Neural Network...\n",
    "    - (Unsupervised) K-means, Hierachical Clustering, Principle Component Analysis, Manifold Learning (MDS, IsoMap, Diffusion Map, tSNE), Auto Encoder...\n",
    "    \n",
    "\n",
    "- **Algorithms**: Gradient Descent, Stochastic Gradient Descent (SGD), Back Propagation (BP),Expectation–Maximization (EM)...\n",
    "    \n",
    "    \n",
    "For the same **problem**, there may exist multiple **models** to describe it. Given the specific **model**, there might be many different **algorithms** to solve it.\n",
    "\n",
    "Why there is so much diversity? The following two fundamental principles of machine learning may provide theoretical insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**[Bias-Variance Trade-off](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)**: Simple models -- large bias, low variance. Complex models -- low bias, large variance. It's totally possible for a model with a worse fit on training data has a better fit on test data.\n",
    "\n",
    "<div>\n",
    "<img src=\"./img_10/bias_and_fitting.png\">\n",
    "</div>\n",
    "\n",
    "**[No Free Lunch Theorem](https://analyticsindiamag.com/what-are-the-no-free-lunch-theorems-in-data-science/#:~:text=Once%20Upon%20A%20Time,that%20they%20brought%20a%20drink)**: (in plain language) There is no one model that works best for every problem. (more quantitatively) Any two models are equivalent when their performance averaged across all possible problems. --Even true for [optimization algorithms](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions of OLS:  MLE, Regularization, Ridge Regression and LASSO\n",
    "\n",
    "*Note: The detailed mathematical derivations below are optional material. What you need to know for quizzes/exams is:*\n",
    "\n",
    "1) what is the relation between MLE (most likelihood estimation) and the loss function in OLS regression (ordinary least-square) ;\n",
    "\n",
    "2) the basic concepts of Ridge regression and LASSO ;\n",
    "\n",
    "3) where does the additional regularization terms in the loss function of Ridge and LASSO come from ;\n",
    "\n",
    "4) which model has the best performance on training/test dataset? (or is there any theoretical guarantee?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Likelihood Estimation (MLE) and loss function in OLS\n",
    "We already known what the loss function looks like in OLS. Here we first provide a mathematical explanation of this loss function from the perspective of \n",
    "Most Likelihood Estimation (MLE).\n",
    "\n",
    "Recall that in linear regression, our **model assumption** is \n",
    "\n",
    "$$y^{(i)}=\\tilde{x}^{(i)}\\beta+\\epsilon^{(i)}, i = 1,2,.., N$$ \n",
    "\n",
    "Now we further **assume** that residuals or errors $\\epsilon^{(i)}$ are as independent Gaussian random variables with identical distribution $\\mathcal{N}(0,\\sigma^{2})$ which has mean 0 and standard deviation $\\sigma$.\n",
    "\n",
    "From the density function of Gaussian distribution, the probability to observe $\\epsilon^{(i)}$ within the small interval $[z,z+\\Delta z]$ is roughly $$\\mathbb{P}(z<\\epsilon^{(i)}<z+\\Delta z) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{z^2}{2\\sigma^2}}\\right)\\Delta z.$$\n",
    "\n",
    "From the data, we know indeed $z=y^{(i)}-\\tilde{x}^{(i)}\\beta$. Therefore, given $x^{(i)}$ as fixed, the conditional probability density (likelihood) to observe $y^{(i)}$ is roughly $$l(y^{(i)}|\\beta; x^{(i)})=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{(y^{(i)}-\\tilde{x}^{(i)}\\beta)^2}{2\\sigma^2}}\\right).$$\n",
    "\n",
    "Using the *independence* assumption, the overall likelihood to observe the response data $y^{(i)} \\, (i=1,2,...,N)$ is \n",
    "\n",
    "$$\\mathcal{P}(y^{(i)},1\\leq i\\leq N|\\beta; x^{(i)})=\\prod_{i=1}^{N}l(y^{(i)}|\\beta; x^{(i)})$$\n",
    "\n",
    "The famous **Maximum Likelihood Estimation (MLE)** theory in statistics **assumes** that we aim to find the unknown parameter $\\beta$ that maximizes the conditional distribution $\\mathcal{P}(\\beta \\,| \\, x^{(i)},y^{(i)},1\\leq i\\leq N)$ by treating $x^{(i)}$ and $y^{(i)}$ as fixed numbers. \n",
    "\n",
    "Equivalently (using the properties of logarithms), as the function of $\\beta$, we can maximize $$\\ln \\mathcal{P}= \\sum_{i=1}^{N}\\ln l(y^{(i)}|\\beta;x^{(i)}).$$ \n",
    "\n",
    "By removing the constants, we finally arrives at the **minimization** problem of $L^{2}$ loss function (whose difference with **MSE -- mean squared error** is only up to the factor 1/N)\n",
    "$$N\\cdot\\text{MSE} = N\\cdot L(\\beta)=\\sum_{i=1}^{N}(y^{(i)}-\\tilde{x}^{(i)}\\beta)^{2}= ||Y-\\tilde{X}\\beta||_{2}^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP (instead of MLE) Estimation in Bayesian Statistics\n",
    "\n",
    "**Recall** the likelihood function -- we interpret it as the probability of observing the response data, given the parameter $\\beta$ as fixed, i.e. conditional probability\n",
    "\n",
    "$$\\mathcal{P}(y^{(i)},1\\leq i\\leq N|x^{(i)},\\beta)=\\prod_{i=1}^{N}l(y^{(i)}|x^{(i)},\\beta)$$\n",
    "\n",
    "Now we take a Bayesian approach -- assume $\\beta$ is the random variable with **prior distirbution** $\\mathcal{P}(\\beta)$. Then the **posterior distribution** of $\\beta$ given the data is  \n",
    "\n",
    "\n",
    "$$\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)\\propto \\mathcal{P}(\\beta)\\mathcal{P}(y^{(i)},1\\leq i\\leq N|\\beta,x^{(i)}).$$\n",
    "\n",
    "\n",
    "The **Bayesian** estimation aims to maximize the posterior distribution. It is formally termed as **Maximum A-Posteriori Estimation (MAP)**. Note that \n",
    "\n",
    "\n",
    "$$\\text{argmax}_{\\beta}\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)=\\text{argmax}_{\\beta}\\ln\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)$$\n",
    "\n",
    "In other words, $\\beta$ maximizes one function if and only if it maximizes the other, and vice versa.\n",
    "\n",
    "\n",
    "- Case 1: The prior distribution $\\mathcal{P}(\\beta_{i}=x)\\propto \\exp(-x^{2}), i\\geq 1$ is Gaussian-like, and different $\\beta_{i}$ are independent. Now the minimization problem becomes \n",
    "\n",
    "    $$\\min_{\\beta} ||Y-\\tilde{X}\\beta||_{2}^2+\\lambda||\\beta||_{2}^{2}.$$\n",
    "\n",
    "    here $||\\beta||_{2}^{2}=\\sum_{i=1}^{p}\\beta_{i}^{2}.$\n",
    "    This is called **Ridge Regression**. \n",
    "    \n",
    "    \n",
    "- Case 2: The prior distribution $\\mathcal{P}(\\beta_{i}=x)\\propto \\exp(-|x|), i\\geq 1$ is double-exponential (Laplace) like, and different $\\beta_{i}$ are independent. Now the minimization problem becomes \n",
    "\n",
    "    $$\\min_{\\beta} ||Y-\\tilde{X}\\beta||_{2}^2+\\lambda\\sum_{i=1}^{p}|\\beta_{i}|$$\n",
    "    \n",
    "    This is called [**LASSO Regression**](https://en.wikipedia.org/wiki/Lasso_(statistics)).\n",
    "    \n",
    "    \n",
    "    \n",
    "In general, these additional terms are called the **regularization terms**. In statistics, regularization is equivalent to Bayesian prior. Here $\\lambda$ is the adjustable parameter in algorithm --  its choice is empirical while sometimes very important for model performance (where the word \"alchemy\" arises in machine learning) Roughly it controls the **complexity** of the model:\n",
    "\n",
    "- If $\\lambda\\to\\infty$, we have $\\beta_{i}\\to 0 (i\\geq 1)$ and $\\beta_{0} = \\bar{y}$. (no complexity in the model)\n",
    "- If $\\lambda\\to 0 $, it will yield the same results with OLS. (Same complexity in model as OLS)\n",
    "\n",
    "Why control the complexity? Recall the bias-variance tradeoff -- sometimes reducing the complexity of a model **might** help to improve performance in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm consideration \n",
    "\n",
    "The optimization for ridge regression is similar to OLS -- try to derive the analytical solution your self. The optimization for LASSO is [non-trival](https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf) and is the important topic in convex optimization. \n",
    "\n",
    "### Prediction in Test Data\n",
    "\n",
    "Now from the same training dataset, we have three $\\beta$ estimated from three different models, namely $\\hat{\\beta}^{OLS},\\hat{\\beta}^{Ridge},\\hat{\\beta}^{Lasso}$ because they are the minimizers of three different loss functions.\n",
    "\n",
    "Given the new observation called $x^{(test)}$, the formal expression of predictions from different methods are the same $$\\hat{y}^{(test)}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x^{(test)}_{1}+..+\\hat{\\beta}_{p}x^{(test)}_{p} = \\tilde{x}^{(test)}\\hat{\\beta}.$$\n",
    "\n",
    "The **only** difference is what $\\hat{\\beta}$ we use. Of course, the corresponded prediction values are also different.\n",
    "\n",
    "### Model Performance Evaluation\n",
    "\n",
    "   - Mean Square Error (MSE) -- the lower, the better (in test data):  $\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}$\n",
    "   - R-squared (coefficient of determination, $R^{2}$) -- the larger, the better (in test data): $1 - \\frac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{MSE}{Var(Y)}$\n",
    "   \n",
    "\n",
    "Question: What about on the training dataset?\n",
    "\n",
    "Conclusion: **By definition**, compared with Ridge or LASSO regression, OLS **will be sure** to have the smallest MSE (hence largest $R^{2}$) on **training dataset**. Think why!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Diabetes Dataset\n",
    "\n",
    "We use the [scikit-learn package](https://scikit-learn.org/stable/index.html) to load the data and run regression. More tutorials about linear models can be [found here](https://scikit-learn.org/stable/modules/linear_model.html).\n",
    "\n",
    "Data from [this paper](https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf) by Professor [Robert Tibshirani et al](https://statweb.stanford.edu/~tibs/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X,y= datasets.load_diabetes(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_diabetes in module sklearn.datasets._base:\n",
      "\n",
      "load_diabetes(*, return_X_y=False, as_frame=False)\n",
      "    Load and return the diabetes dataset (regression).\n",
      "    \n",
      "    ==============   ==================\n",
      "    Samples total    442\n",
      "    Dimensionality   10\n",
      "    Features         real, -.2 < x < .2\n",
      "    Targets          integer 25 - 346\n",
      "    ==============   ==================\n",
      "    \n",
      "    .. note::\n",
      "       The meaning of each feature (i.e. `feature_names`) might be unclear\n",
      "       (especially for `ltg`) as the documentation of the original dataset is\n",
      "       not explicit. We provide information that seems correct in regard with\n",
      "       the scientific literature in this field of research.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <diabetes_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target columns.\n",
      "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "        DataFrames or Series as described below.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : {ndarray, dataframe} of shape (442, 10)\n",
      "            The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "            DataFrame.\n",
      "        target: {ndarray, Series} of shape (442,)\n",
      "            The regression target. If `as_frame=True`, `target` will be\n",
      "            a pandas Series.\n",
      "        feature_names: list\n",
      "            The names of the dataset columns.\n",
      "        frame: DataFrame of shape (442, 11)\n",
      "            Only present when `as_frame=True`. DataFrame with `data` and\n",
      "            `target`.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "        data_filename: str\n",
      "            The path to the location of the data.\n",
      "        target_filename: str\n",
      "            The path to the location of the target.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "        Returns a tuple of two ndarray of shape (n_samples, n_features)\n",
      "        A 2D array with each row representing one sample and each column\n",
      "        representing the features and/or target of a given sample.\n",
      "        .. versionadded:: 0.18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.load_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fr,y_fr= datasets.load_diabetes(return_X_y = True, as_frame=True) #not required for Regression\n",
    "X_fr #This just gives us a better description of the values in X\n",
    "#The categories themselves are normalized, so they are not simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      151.0\n",
       "1       75.0\n",
       "2      141.0\n",
       "3      206.0\n",
       "4      135.0\n",
       "       ...  \n",
       "437    178.0\n",
       "438    104.0\n",
       "439    132.0\n",
       "440    220.0\n",
       "441     57.0\n",
       "Name: target, Length: 442, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the training and test dataset by random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "#do not use random_state unless specifically asked in an assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 10)\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets.\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Square (OLS) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg_ols = linear_model.LinearRegression()\n",
    "reg_ols.fit(X_train,y_train) # train the parameters in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_decision_function',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_more_tags',\n",
       " '_preprocess_data',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_residues',\n",
       " '_set_intercept',\n",
       " '_validate_data',\n",
       " 'coef_',\n",
       " 'copy_X',\n",
       " 'fit',\n",
       " 'fit_intercept',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'n_features_in_',\n",
       " 'n_jobs',\n",
       " 'normalize',\n",
       " 'positive',\n",
       " 'predict',\n",
       " 'rank_',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'singular_']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(reg_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  19.92576904, -262.55453086,  509.19112446,  336.09693678,\n",
       "       -849.29530342,  480.22076125,  120.68418641,  236.71853501,\n",
       "        716.61035542,   70.41045019])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([143.06621271, 177.70923973, 134.80159283, 288.66523611,\n",
       "       123.58429291,  96.64399491, 252.70865552, 183.51563317,\n",
       "        93.96508916, 109.83316004,  98.04648824, 168.61502622,\n",
       "        58.09759262, 206.5896178 , 102.4078438 , 130.25693511,\n",
       "       218.0570909 , 245.9207401 , 193.24351477, 214.36945188,\n",
       "       208.82778064,  90.55665059,  74.15304744, 187.1216387 ,\n",
       "       156.36442036, 157.46376883, 184.17736744, 177.18027887,\n",
       "        52.24263585, 110.66673778, 174.05918425,  90.89850309,\n",
       "       133.07968763, 183.22988596, 173.93725211, 189.85248233,\n",
       "       125.86458581, 121.53390004, 148.94895292,  60.82842472,\n",
       "        76.36312191, 106.40220555, 162.20473499, 153.15077269,\n",
       "       174.23003255])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ols = reg_ols.predict(X_test) # generate predictions in test dataset\n",
    "y_pred_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2743.8800467688466 0.5514251914993502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_ols = mean_squared_error(y_test, y_pred_ols)\n",
    "R2_ols =  reg_ols.score(X_test,y_test) # the R-squared value -- how good is the fitting in test dataset?\n",
    "print(mse_ols,R2_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  21.70557246 -252.8105591   507.97196544  328.21420703 -280.47609687\n",
      "   37.89517179 -127.46013757  163.28415598  497.87046059   77.00701528]\n",
      "2735.677504142067 0.5527661590071533\n"
     ]
    }
   ],
   "source": [
    "reg_ridge = linear_model.Ridge(alpha=.02) # alpha is proportional to the lambda above -- only up to the constant\n",
    "reg_ridge.fit(X_train,y_train)\n",
    "print(reg_ridge.coef_)\n",
    "\n",
    "y_pred_ridge = reg_ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "R2_ridge =  reg_ridge.score(X_test,y_test)\n",
    "print(mse_ridge,R2_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.         -173.66792464  510.09537263  286.77901824  -64.43166585\n",
      "   -0.         -226.79324775    0.          453.3557073    44.84749075]\n",
      "2636.3323830017634 0.569007291247414\n"
     ]
    }
   ],
   "source": [
    "reg_lasso = linear_model.Lasso(alpha=0.1) # alpha is proportional to the lambda above -- only up to the constant\n",
    "reg_lasso.fit(X_train,y_train)\n",
    "print(reg_lasso.coef_)\n",
    "\n",
    "y_pred_lasso = reg_lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "R2_lasso =  reg_lasso.score(X_test,y_test)\n",
    "print(mse_lasso,R2_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5125152248773208\n",
      "0.5102072320833588\n",
      "0.5024076500883519\n"
     ]
    }
   ],
   "source": [
    "print(reg_ols.score(X_train,y_train)) # note that we calculate score on TRAINING dataset\n",
    "print(reg_ridge.score(X_train,y_train))\n",
    "print(reg_lasso.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, OLS has the smallest MSE (largest R-squared) on **training dataset**. What about on the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "alphas = np.logspace(-5, -1, 20)\n",
    "for alpha in alphas:\n",
    "    reg_lasso.set_params(alpha=alpha) # change the parameter of reg_lasso\n",
    "    reg_lasso.fit(X_train, y_train)\n",
    "    train_errors.append(reg_lasso.score(X_train, y_train))\n",
    "    test_errors.append(reg_lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-05 1.62377674e-05 2.63665090e-05 4.28133240e-05\n",
      " 6.95192796e-05 1.12883789e-04 1.83298071e-04 2.97635144e-04\n",
      " 4.83293024e-04 7.84759970e-04 1.27427499e-03 2.06913808e-03\n",
      " 3.35981829e-03 5.45559478e-03 8.85866790e-03 1.43844989e-02\n",
      " 2.33572147e-02 3.79269019e-02 6.15848211e-02 1.00000000e-01]\n"
     ]
    }
   ],
   "source": [
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x288044c4910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(dpi=100)\n",
    "plt.semilogx(alphas,train_errors,label = 'train R2')\n",
    "plt.semilogx(alphas,test_errors,label = 'test R2')\n",
    "plt.xlabel('alpha')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, a good model in training dataset does not mean it's a good model in the final test dataset. Then how can we use the best model (i.e. model with best regularization parameters)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "What if we don't know the true labels in test, but the performance in test is so important to us so that we really want to select a model with greater confidence with traning dataset?\n",
    "\n",
    "As discussed previously, we can use traning dataset to make 10 \"quizzes\" (each \"quiz\" is called a validation dataset), and let the three models to compete based on the 10 \"competitions\". This is called 10-fold cross-validation.\n",
    "\n",
    "For the more detailed discussion and distinguishment between training, validation and test datasets, you can refer to this [wikipedia link](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Test_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_lasso = cross_val_score(reg_lasso, X_train, y_train, cv=10) # cross-validation function in sklearn\n",
    "scores_ridge = cross_val_score(reg_ridge, X_train, y_train, cv=10)\n",
    "scores_ols = cross_val_score(reg_ols, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24777555 0.59326777 0.47897959 0.5352791  0.32317178 0.47569164\n",
      " 0.6518041  0.56942576 0.25184587 0.36446431]\n",
      "[0.24342237 0.57522902 0.52325584 0.53031117 0.34021405 0.48194162\n",
      " 0.6585968  0.57423334 0.24263773 0.33362724]\n",
      "[0.23604669 0.57037558 0.53700808 0.52611281 0.34264557 0.49282279\n",
      " 0.66256801 0.57878559 0.19975324 0.34375095]\n"
     ]
    }
   ],
   "source": [
    "print(scores_lasso)\n",
    "print(scores_ridge)\n",
    "print(scores_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cross_val_score in module sklearn.model_selection._validation:\n",
      "\n",
      "cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)\n",
      "    Evaluate a score by cross-validation.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : estimator object implementing 'fit'\n",
      "        The object to use to fit the data.\n",
      "    \n",
      "    X : array-like of shape (n_samples, n_features)\n",
      "        The data to fit. Can be for example a list, or an array.\n",
      "    \n",
      "    y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "        The target variable to try to predict in the case of\n",
      "        supervised learning.\n",
      "    \n",
      "    groups : array-like of shape (n_samples,), default=None\n",
      "        Group labels for the samples used while splitting the dataset into\n",
      "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "        instance (e.g., :class:`GroupKFold`).\n",
      "    \n",
      "    scoring : str or callable, default=None\n",
      "        A str (see model evaluation documentation) or\n",
      "        a scorer callable object / function with signature\n",
      "        ``scorer(estimator, X, y)`` which should return only\n",
      "        a single value.\n",
      "    \n",
      "        Similar to :func:`cross_validate`\n",
      "        but only a single metric is permitted.\n",
      "    \n",
      "        If `None`, the estimator's default scorer (if available) is used.\n",
      "    \n",
      "    cv : int, cross-validation generator or an iterable, default=None\n",
      "        Determines the cross-validation splitting strategy.\n",
      "        Possible inputs for cv are:\n",
      "    \n",
      "        - `None`, to use the default 5-fold cross validation,\n",
      "        - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "        - :term:`CV splitter`,\n",
      "        - An iterable that generates (train, test) splits as arrays of indices.\n",
      "    \n",
      "        For `int`/`None` inputs, if the estimator is a classifier and `y` is\n",
      "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "        other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "        with `shuffle=False` so the splits will be the same across calls.\n",
      "    \n",
      "        Refer :ref:`User Guide <cross_validation>` for the various\n",
      "        cross-validation strategies that can be used here.\n",
      "    \n",
      "        .. versionchanged:: 0.22\n",
      "            `cv` default value if `None` changed from 3-fold to 5-fold.\n",
      "    \n",
      "    n_jobs : int, default=None\n",
      "        Number of jobs to run in parallel. Training the estimator and computing\n",
      "        the score are parallelized over the cross-validation splits.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "    \n",
      "    verbose : int, default=0\n",
      "        The verbosity level.\n",
      "    \n",
      "    fit_params : dict, default=None\n",
      "        Parameters to pass to the fit method of the estimator.\n",
      "    \n",
      "    pre_dispatch : int or str, default='2*n_jobs'\n",
      "        Controls the number of jobs that get dispatched during parallel\n",
      "        execution. Reducing this number can be useful to avoid an\n",
      "        explosion of memory consumption when more jobs get dispatched\n",
      "        than CPUs can process. This parameter can be:\n",
      "    \n",
      "            - ``None``, in which case all the jobs are immediately\n",
      "              created and spawned. Use this for lightweight and\n",
      "              fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "    \n",
      "            - An int, giving the exact number of total jobs that are\n",
      "              spawned\n",
      "    \n",
      "            - A str, giving an expression as a function of n_jobs,\n",
      "              as in '2*n_jobs'\n",
      "    \n",
      "    error_score : 'raise' or numeric, default=np.nan\n",
      "        Value to assign to the score if an error occurs in estimator fitting.\n",
      "        If set to 'raise', the error is raised.\n",
      "        If a numeric value is given, FitFailedWarning is raised.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    scores : ndarray of float of shape=(len(list(cv)),)\n",
      "        Array of scores of the estimator for each run of the cross validation.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn import datasets, linear_model\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> diabetes = datasets.load_diabetes()\n",
      "    >>> X = diabetes.data[:150]\n",
      "    >>> y = diabetes.target[:150]\n",
      "    >>> lasso = linear_model.Lasso()\n",
      "    >>> print(cross_val_score(lasso, X, y, cv=3))\n",
      "    [0.33150734 0.08022311 0.03531764]\n",
      "    \n",
      "    See Also\n",
      "    ---------\n",
      "    cross_validate : To run cross-validation on multiple metrics and also to\n",
      "        return train scores, fit times and score times.\n",
      "    \n",
      "    cross_val_predict : Get predictions from each split of cross-validation for\n",
      "        diagnostic purposes.\n",
      "    \n",
      "    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "        loss function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso</th>\n",
       "      <th>ols</th>\n",
       "      <th>ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.236047</td>\n",
       "      <td>0.243422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.593268</td>\n",
       "      <td>0.570376</td>\n",
       "      <td>0.575229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.478980</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.535279</td>\n",
       "      <td>0.526113</td>\n",
       "      <td>0.530311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.323172</td>\n",
       "      <td>0.342646</td>\n",
       "      <td>0.340214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.475692</td>\n",
       "      <td>0.492823</td>\n",
       "      <td>0.481942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.651804</td>\n",
       "      <td>0.662568</td>\n",
       "      <td>0.658597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.569426</td>\n",
       "      <td>0.578786</td>\n",
       "      <td>0.574233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.251846</td>\n",
       "      <td>0.199753</td>\n",
       "      <td>0.242638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.364464</td>\n",
       "      <td>0.343751</td>\n",
       "      <td>0.333627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lasso       ols     ridge\n",
       "0  0.247776  0.236047  0.243422\n",
       "1  0.593268  0.570376  0.575229\n",
       "2  0.478980  0.537008  0.523256\n",
       "3  0.535279  0.526113  0.530311\n",
       "4  0.323172  0.342646  0.340214\n",
       "5  0.475692  0.492823  0.481942\n",
       "6  0.651804  0.662568  0.658597\n",
       "7  0.569426  0.578786  0.574233\n",
       "8  0.251846  0.199753  0.242638\n",
       "9  0.364464  0.343751  0.333627"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "scores_all = pd.DataFrame({\"lasso\": scores_lasso,\"ols\": scores_ols, \"ridge\":scores_ridge})\n",
    "scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scores_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides mean and standard deviation, we can also use the [boxplot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "sns.boxplot(data = scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso</th>\n",
       "      <th>ols</th>\n",
       "      <th>ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.449171</td>\n",
       "      <td>0.448987</td>\n",
       "      <td>0.450347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.144468</td>\n",
       "      <td>0.157290</td>\n",
       "      <td>0.148598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.199753</td>\n",
       "      <td>0.242638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.333495</td>\n",
       "      <td>0.342922</td>\n",
       "      <td>0.335274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.477336</td>\n",
       "      <td>0.509468</td>\n",
       "      <td>0.502599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.560889</td>\n",
       "      <td>0.562034</td>\n",
       "      <td>0.563253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.651804</td>\n",
       "      <td>0.662568</td>\n",
       "      <td>0.658597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lasso        ols      ridge\n",
       "count  10.000000  10.000000  10.000000\n",
       "mean    0.449171   0.448987   0.450347\n",
       "std     0.144468   0.157290   0.148598\n",
       "min     0.247776   0.199753   0.242638\n",
       "25%     0.333495   0.342922   0.335274\n",
       "50%     0.477336   0.509468   0.502599\n",
       "75%     0.560889   0.562034   0.563253\n",
       "max     0.651804   0.662568   0.658597"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the final judgement is still in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.569007291247414"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_lasso.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5527661590071533"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ridge.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5514251914993502"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ols.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Use cross-validation to select the `alpha` parameter in LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#test_errors = list()\n",
    "alphas = np.logspace(-5, -1, 20)\n",
    "train_errors = np.zeros_like(alphas)\n",
    "for ind, alpha in enumerate(alphas):\n",
    "    reg_lasso.set_params(alpha=alpha) # change the parameter of reg_lasso\n",
    "    #reg_lasso.fit(X_train, y_train)\n",
    "    scores_lasso = cross_val_score(reg_lasso, X_train, y_train, cv=10)\n",
    "    #train_errors.append(reg_lasso.score(X_train, y_train))\n",
    "    #score_mean = scores_lasso.mean()\n",
    "    train_errors[ind]= scores_lasso.mean()\n",
    "    #test_errors.append(reg_lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44898873 0.44898985 0.44899167 0.44899459 0.4489993  0.44900684\n",
      " 0.44901877 0.44903735 0.44906542 0.44910546 0.44919526 0.44935973\n",
      " 0.44972176 0.44992911 0.44901409 0.44909977 0.45044024 0.45092872\n",
      " 0.45140977 0.44917055]\n",
      "0.4514097740599895\n",
      "18\n",
      "0.06158482110660261\n"
     ]
    }
   ],
   "source": [
    "print(train_errors)\n",
    "print(train_errors.max())\n",
    "print(train_errors.argmax())\n",
    "print(alphas[train_errors.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.         -202.49710284  513.13597299  303.12781379 -101.29843297\n",
      "   -0.         -237.82654836    0.          474.55221916   58.20204323]\n",
      "2641.0204812969946 0.5682408718853533\n"
     ]
    }
   ],
   "source": [
    "reg_lasso_new = linear_model.Lasso(alpha=alphas[train_errors.argmax()]) # alpha is proportional to the lambda above -- only up to the constant\n",
    "reg_lasso_new.fit(X_train,y_train)\n",
    "print(reg_lasso_new.coef_)\n",
    "\n",
    "y_pred_lasso_new = reg_lasso_new.predict(X_test)\n",
    "mse_lasso_new = mean_squared_error(y_test, y_pred_lasso_new)\n",
    "R2_lasso_new =  reg_lasso_new.score(X_test,y_test)\n",
    "print(mse_lasso_new,R2_lasso_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Reading Suggestions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ISLR: Chapter 2,3,6\n",
    "- ESL: Chapter 1,2,3\n",
    "- PML: Chapter 1,2,3,4,7,11\n",
    "- DL: Chapter 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
