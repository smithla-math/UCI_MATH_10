{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11: Logistic Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problem, the response variables $y$ are discrete, representing different catagories. \n",
    "\n",
    "**Why not use linear regression for classification problem?**\n",
    "- The problem for range of $y$\n",
    "- The inappropriate **MSE** loss function, especially for multi-class classification. It does not make sense to assume miss-classify 9 for 1 will yield a larger penalty than 7 for 1.\n",
    "- There's no order in the $y$ in **classification** -- they are just categories (imagine Iris flower, we can permute the label number as we like, while the permutation will definitely affect **regression** results)\n",
    "\n",
    "Therefore for classification problem, we may want to:\n",
    "- replace the mapping assumption between $y$ and $x$\n",
    "- replace the loss function in regression\n",
    "\n",
    "In this section, we're going to learn [**logistic regression**](https://en.wikipedia.org/wiki/Logistic_regression), which is a linear **classification** method and a direct generalization of linear regression. We will learn more classification models in the next section.\n",
    "\n",
    "## Binary Classification\n",
    "\n",
    "For simplicity, we will first introduce the **binary classification case** -- $y$ has only two categories, denoted as $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-setup of Logistic Regression (this is a classification model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption 1**: Dependent on the variable $x$, the response variable $y$ has different **probabilities** to take value in 0 or 1. Instead of predicting exact value of 0 or 1, we are actually predicting the **probabilities**.\n",
    "\n",
    "**Assumption 2**: Logistic function assumption. Given $x$, what is the probability to observe $y=1$?\n",
    "\n",
    "$$P(y=1|\\mathbf{x})= f(\\mathbf{x};\\mathbf{\\beta}) = \\frac{1}{1 + \\exp(-\\tilde{x}\\mathbf{\\beta})}\n",
    "=: \\sigma(\\tilde{x}\\mathbf{\\beta}). $$\n",
    "\n",
    "where $\\sigma(z)=\\frac{1}{1+\\exp{(-z)}}$ is called [standard logistic function](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression), or sigmoid function in deep learning. Recall that $\\beta\\in\\mathbb{R}^{p+1}$ and $\\tilde{x}$ is the \"augmented\" sample with first element one to incorporate intercept in the linear function.\n",
    "\n",
    "**Equivalent expression**:\n",
    "  - Denote $p = P(y=1|\\mathbf{x})$, then we can write in linear form (the LHS is called **odds ratio** in statistics)\n",
    "  $$\\ln\\frac{p}{1-p}=\\tilde{x}\\beta$$\n",
    "  - Since $y$ only takes value in 0 or 1, we choose our exponents to be **indicators** of y. We have\n",
    "  $$P(y|\\mathbf{x},\\beta) = f(\\mathbf{x};\\beta)^y \\big(1 - f(\\mathbf{x};\\beta) \\big)^{1-y}$$\n",
    "  - Note: for conditional probability, $|$ and $;$ are interchangable and mean the same thing.\n",
    "  \n",
    "**MLE (Maximum Likelihood Estimation)**\n",
    "\n",
    "Assume the samples are independent. The overall probability to witness the whole training dataset\n",
    "$$\n",
    "{\\begin{aligned}\n",
    "&P(\\mathbf{y}\\; | \\; \\mathbf{X};\\beta )\\\\\n",
    "=&\\prod _{i=1}^N P\\left(y^{(i)}\\mid \\mathbf{x}^{(i)};\\beta\\right)\\\\\n",
    "=&\\prod_{i=1}^N f\\big(\\mathbf{x}^{(i)};\\beta \\big)^{y^{(i)}} \n",
    "\\Big(1-f\\big(\\mathbf{x}^{(i)};\\beta\\big) \\Big)^{\\big(1-y^{(i)}\\big)}.\n",
    "\\end{aligned}}\n",
    "$$\n",
    "\n",
    "By maximizing the logarithm of likelihood function, then we derive the **loss function** to be minimized\n",
    "$$\n",
    "L (\\beta) = L (\\beta; X,\\mathbf{y}) = - \\frac{1}{N}\\sum_{i=1}^N \n",
    "\\Bigl\\{y^{(i)} \\ln\\big( f(\\mathbf{x}^{(i)};\\beta) \\big) \n",
    "+ (1 - y^{(i)}) \\ln\\big( 1 - f(\\mathbf{x}^{(i)};\\beta) \\big) \\Bigr\\}.\n",
    "$$\n",
    "\n",
    "The loss function also has clear probabilistic interpretations. Given $i$-th sample, the vector of true labels $(y^{(i)},1-y^{(i)})$ can also be viewed as the probability distribution. Then the loss function is the mean of all [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) across samples, i.e. **\"distance\" between observed sample probability distribution and modelled probability distribution** via logistic model.\n",
    "\n",
    "**Remark**: here we derive the loss function via MLE. Of course from the experience of linear regression, we know that we can also use MAP (bayesian approach), where the regularization term of $\\beta$ can be naturally introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the gradient (left as exercise -- if you like)\n",
    "$$\n",
    "\\frac{\\partial L (\\beta)}{\\partial \\beta_{k}} =\\frac{1}{N}\\sum_{i=1}^N  \\big(\\sigma(\\tilde{x}^{(i)}\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)}_k.\n",
    "$$\n",
    "\n",
    "In vector form:\n",
    "$$\n",
    "\\nabla_{\\beta} \\big( L (\\beta) \\big) = \\frac{1}{N} \\sum_{i=1}^N \\big(\\sigma(\\tilde{x}^{(i)}\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)} \n",
    "=\\frac{1}{N}\\sum_{i=1}^N \\big( f(\\mathbf{x}^{(i)};\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)}$$ \n",
    "\n",
    "The last expression in the above line can be further represented as a row vector times a matrix:\n",
    "$$\\nabla_{\\beta} \\big( L (\\beta) \\big) = \\frac{1}{N}\\big( f(\\mathbf{x}^{(1)};\\beta)  - y^{(1)}, \\hspace{1pc} f(\\mathbf{x}^{(2)};\\beta)  - y^{(2)}, \\hspace{1pc} \\cdots \\hspace{1pc} f(\\mathbf{x}^{(N)};\\beta)  - y^{(N)} \\big)\\tilde{X}.\n",
    "$$\n",
    "\n",
    "However, this is a nonlinear function of $\\beta$, indicating that we cannot derive something like \"normal equations\" in OLS. The solution here is [numerical optimization](https://github.com/Jaewan-Yun/optimizer-visualization).\n",
    "\n",
    "The simplest algorithm in optimization is [gradient descent (GD)](https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point.) We create a sequence of vectors $\\{\\beta^0, \\beta^1, \\beta^2, ... \\}$ using the following recursive rule: $$\\beta^{k+1}=\\beta^{k}-\\eta\\nabla L(\\beta^{k}).$$\n",
    "\n",
    "Here the step size $\\eta$ (eta) is also called **learning rate** in machine learning. Note that it is indeed the Euler's scheme to solve the ODE (for a 1-variable version, see [Notes on Diffy Q's Section 1.7](https://www.jirka.org/diffyqs/html/numer_section.html)): $$\\dot{\\beta} = -\\nabla L(\\beta).$$\n",
    "\n",
    "By setting certain stopping criterion for GD, we think that we have approximated the optimized solution $\\hat{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions and Evaluation of Performance\n",
    "\n",
    "Now with the estimated $\\hat{\\beta}$ and given a new data $x^{new}$, we calculate the probability that $y^{new}=1$ as $f(\\mathbf{x};\\mathbf{\\beta})$. If is greater than 0.5, we assign that $y^{new}=1$. \n",
    "\n",
    "For the test dataset, the **accuracy** is defined as ratio of number of correct predictions to the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogisticRegression_binary():\n",
    "    \"\"\" Logistic Regression classifier -- this only works for the binary case.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        \n",
    "        # learning rate can also be in the fit method\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "\n",
    "    def fit(self, data, y, n_iterations = 1000):\n",
    "        \"\"\" \n",
    "        don't forget the document string in methods\n",
    "        \"\"\"\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        beta  = np.zeros(np.shape(X)[1]) # initialize beta, same as initial condition beta_0 = [0,0,0,0,0,0,0,...0] , can be other choices\n",
    "\n",
    "        for k in range(n_iterations):\n",
    "            dbeta = self.loss_gradient(beta,X,y) # write another function to compute gradient\n",
    "            beta = beta - eta * dbeta # the formula of GD\n",
    "            # this step is optional -- just for inspection purposes\n",
    "            if k % 500 == 0: # print loss every 500 steps\n",
    "                print(\"loss after\", k+1, \"iterations is: \", self.loss(beta,X,y))\n",
    "        \n",
    "        self.coeff = beta\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        beta = self.coeff # the estimated beta\n",
    "        y_pred = np.round(self.sigmoid(np.dot(X,beta))).astype(int) # >0.5: ->1 else,->0 -- note that we always use Numpy universal functions when possible\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, data, y_true):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        y_pred = self.predict(data)\n",
    "        acc = np.mean(y_pred == y_true) # number of correct predictions/N\n",
    "        return acc\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def loss(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))\n",
    "        loss_value = np.log(f_value + 1e-10) * y + (1.0 - y)* np.log(1 - f_value + 1e-10) #log base 10. Using 1e-10 avoids nan issues\n",
    "        return -np.mean(loss_value)\n",
    "                          \n",
    "    def loss_gradient(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))                  \n",
    "        gradient_value = (f_value - y).reshape(-1,1)*X # this is the hardest expression -- check yourself. It's called Numpy broadcasting\n",
    "        return np.mean(gradient_value, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fr, y_fr = load_breast_cancer(return_X_y = True, as_frame = True) #Optional, not used in the training or testing phases. Just to look at dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "564    0\n",
       "565    0\n",
       "566    0\n",
       "567    0\n",
       "568    1\n",
       "Name: target, Length: 569, dtype: int32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 iterations is:  0.7704000919325609\n",
      "loss after 501 iterations is:  0.3038878556607375\n",
      "loss after 1001 iterations is:  0.2646726705164665\n",
      "loss after 1501 iterations is:  0.2481347924595069\n",
      "loss after 2001 iterations is:  0.23894805957882753\n",
      "loss after 2501 iterations is:  0.23311785521728878\n",
      "loss after 3001 iterations is:  0.22909746536348713\n",
      "loss after 3501 iterations is:  0.2261514996674705\n",
      "loss after 4001 iterations is:  0.22388601401780295\n",
      "loss after 4501 iterations is:  0.2220728516137011\n",
      "loss after 5001 iterations is:  0.2205722111378522\n",
      "loss after 5501 iterations is:  0.21929462157026378\n",
      "loss after 6001 iterations is:  0.21818078310948252\n",
      "loss after 6501 iterations is:  0.21719023774728446\n",
      "loss after 7001 iterations is:  0.21629469731306186\n",
      "loss after 7501 iterations is:  0.21547395937965438\n",
      "loss after 8001 iterations is:  0.21471332436186455\n",
      "loss after 8501 iterations is:  0.21400191582326006\n",
      "loss after 9001 iterations is:  0.21333156155309693\n",
      "loss after 9501 iterations is:  0.21269603244872287\n",
      "CPU times: total: 1.09 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg = myLogisticRegression_binary(learning_rate=1e-5)\n",
    "lg.fit(X_train,y_train,n_iterations = 10000) # what about increase n_iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.92156972e-03,  1.35014099e-02,  5.60837090e-03,  6.98616548e-02,\n",
       "        1.63274570e-02,  8.11846822e-05, -3.05842439e-04, -5.82893624e-04,\n",
       "       -2.34074392e-04,  1.52008899e-04,  8.23455085e-05,  1.19749200e-04,\n",
       "        7.36438347e-04, -9.69461818e-04, -2.25609056e-02,  1.60975351e-06,\n",
       "       -8.32345494e-05, -1.14621433e-04, -2.63019486e-05,  7.14323607e-06,\n",
       "       -3.90033063e-06,  1.41429198e-02,  1.95665028e-03,  6.13141823e-02,\n",
       "       -2.82368244e-02,  6.40567132e-05, -1.16724642e-03, -1.62789172e-03,\n",
       "       -4.19315981e-04,  6.01178429e-05,  3.79852714e-06])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukea\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958984375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very normal that our result is different with sklearn. In sklearn [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), by default the loss function is different (they use regularization terms!).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "Note that your final project is a multi-class classification problem\n",
    "\n",
    "### Model \n",
    "Let $\\tilde{x}\\in\\mathbb{R}^{p+1}$ denotes the augmented row vector (one sample). We approximate the probabilities to take value in $K$ classes as\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x};\\mathbf{W}) =\n",
    "\\begin{pmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{W}) \\\\\n",
    "P(y = 2 | \\mathbf{x}; \\mathbf{W}) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | \\mathbf{x}; \\mathbf{W})\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{K}{\\exp\\big(\\tilde{x}\\mathbf{w}_{k}\\big) }}\n",
    "\\begin{pmatrix}\n",
    "\\exp(\\tilde{x}\\mathbf{w}_{1} ) \\\\\n",
    "\\exp(\\tilde{x}\\mathbf{w}_{2} ) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(\\tilde{x}\\mathbf{w}_{K} ) \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "where we have $K$ sets of parameters, $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K$, and the sum factor normalizes the results to be a probability.\n",
    "\n",
    "$\\mathbf{W}$ is an $(p+1)\\times K$ matrix containing all $K$ sets of parameters, obtained by concatenating $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K$ into columns, so that $\\mathbf{w}_k = (w_{k0}, \\dots, w_{kp})^{\\top}\\in \\mathbb{R}^{p+1}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\left(\n",
    "\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_K \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "and $\\tilde{X}\\mathbf{W}$ is valid and useful in vectorized code.\n",
    "\n",
    "**Another Expression**: Introduce the hidden variable $\\mathbf{z} = (z_{1},...,z_{K})$ and define \n",
    "\n",
    "$$ \\mathbf{z}= \\tilde{\\mathbf{x}} \\mathbf{W}$$\n",
    "or element-wise written as \n",
    "$$z_{k} = \\tilde{\\mathbf{x}} \\mathbf{w_{k}}, \\, k = 1,2,...,K$$\n",
    "\n",
    "Then the **predicted probability distribution** can be denoted as \n",
    "\n",
    "$$f(\\mathbf{x};\\mathbf{W}) = \\sigma(z)\\in\\mathbb{R}^{K}$$\n",
    "where vector $\\sigma(\\mathbf{z})$ is called the [soft-max function](https://en.wikipedia.org/wiki/Softmax_function) which is defined as \n",
    "\n",
    "$$ \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}{\\text{ for }}i=1,\\dotsc ,K{\\text{ and }}\\mathbf {z} =(z_{1},\\dotsc ,z_{K})\\in \\mathbb {R} ^{K}$$\n",
    "\n",
    "This is a valid probability distribution with $K$ classes because you can check its element-wise sum is one and each component is positive.\n",
    "\n",
    "This can be assumed as the (degenerate) simplest example of neural network that we're going to learn in later lectures, and that's why some people refer to multi-class logistic regression (also known as **soft-max logistic regression**) as **one-layer neural network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Define the following **indicator function** (and again can be derived from MLE):\n",
    "$$\n",
    "1_{\\{y = k\\}} = 1_{\\{k\\}}(y) = \\delta_{yk} = \\begin{cases}\n",
    "1 & \\text{when } y = k,\n",
    "\\\\[5pt]\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Other notations for indicators: $\\mathbb{1}_{\\{k\\}}(y)$, $\\mathbb{I}_{\\{k\\}}(y)$, ${I}_{\\{k\\}}(y)$\n",
    "\n",
    "Loss function is again using the cross entropy:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L (\\mathbf{W};X,\\mathbf{y})  & = - \\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K\n",
    "\\Bigl\\{ 1_{\\{y^{(i)} = k\\}} \\ln P\\big(y^{(i)}=k | \\mathbf{x}^{(i)} ; \\mathbf{w} \\big) \\Bigr\\}\n",
    "\\\\\n",
    " & = - \\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K\n",
    "\\left\\{1_{\\{y^{(i)} = k\\}} \\ln \\Bigg( \\frac{\\exp(\\tilde{x}^{(i)}\\mathbf{w}_{k})}{\\sum_{m=1}^{K} \n",
    "\\exp\\big(\\tilde{x}^{(i)}\\mathbf{w}_m\\big) }  \\Bigg)\\right\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that for each term in the summation over N (i.e. fix sample i), only one term is non-zero in the sum of K elements due to the indicator function.\n",
    "\n",
    "\n",
    "### Gradient descent\n",
    "After **careful calculation**, the gradient of $L$ with respect the whole $k$-th set of weights is then (in the notation of [Matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus#Scope)):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L }{\\partial \\mathbf{w}_{k}}\n",
    "= \\left( \\frac{\\partial L }{\\partial {w}_{k0}}, \\, \\frac{\\partial L }{\\partial {w}_{k1}}, \\, \\cdots \\frac{\\partial L }{\\partial {w}_{kP}} \\right)^T\n",
    "= \n",
    "\\frac{1}{N}\\sum_{i=1}^N \n",
    "\\left(    \\frac{\\exp(\\tilde{x}^{(i)}\\mathbf{w}_{k})} {\\sum_{m=1}^{K} \n",
    "\\exp(\\tilde{x}^{(i)}\\mathbf{w}_m)} -1_{\\{y^{(i)} = k\\}} \n",
    "\\right)\\tilde{x}^{(i)}\\in\\mathbb{R}^{p+1}.\n",
    "$$\n",
    "\n",
    "In writing the code, it's helpful to make this as the column vector, and stack all the $K$ gradients together as a new matrix $\\mathbf{dW}\\in\\mathbb{R}^{(p+1)\\times K}$. This makes the update of matrix $\\mathbf{W}$ very convenient in gradient descent.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{W}} = dW = \\left(\n",
    "\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\frac{\\partial L }{\\partial \\mathbf{w}_{1}} & \\frac{\\partial L }{\\partial \\mathbf{w}_{2}} & \\cdots & \\frac{\\partial L }{\\partial \\mathbf{w}_{K}} \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "### Prediction\n",
    "The largest estimated probability's class as this sample's predicted label.\n",
    "$$\n",
    "\\hat{y} = \\operatorname{arg}\\max_{j} P\\big(y = j| \\mathbf{x}\\big),\n",
    "$$\n",
    "In other words, we get the class $j$ with the largest conditional probability, a.k.a. likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogisticRegression():\n",
    "    \"\"\" Logistic Regression classifier -- this also works for the multiclass case.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        \n",
    "        # learning rate can also be in the fit method\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "\n",
    "    def fit(self, data, y, n_iterations = 1000):\n",
    "        \"\"\" \n",
    "        don't forget the document string in methods, here and all others!!!\n",
    "        \"\"\"\n",
    "        self.K = max(y)+1 # specify number of classes in y\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        W  = np.zeros((np.shape(X)[1],max(y)+1)) # initialize beta, can be other choices\n",
    "\n",
    "        for k in range(n_iterations):\n",
    "            dW = self.loss_gradient(W,X,y) # write another function to compute gradient\n",
    "            W = W - eta * dW # the formula of GD\n",
    "            # this step is optional -- just for inspection purposes\n",
    "            if k % 500 == 0: # print loss every 500 steps\n",
    "                print(\"loss after\", k+1, \"iterations is: \", self.loss(W,X,y))\n",
    "        \n",
    "        self.coeff = W\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        W = self.coeff # the estimated W\n",
    "        y_pred = np.argmax(self.sigma(X,W), axis =1) # the category with largest probability\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, data, y_true):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        y_pred = self.predict(data)\n",
    "        acc = np.mean(y_pred == y_true) # number of correct predictions/N\n",
    "        return acc\n",
    "    \n",
    "    def sigma(self,X,W): #return the softmax probability\n",
    "        s = np.exp(np.matmul(X,W))\n",
    "        total = np.sum(s, axis=1).reshape(-1,1)\n",
    "        return s/total\n",
    "    \n",
    "    def loss(self,W,X,y):\n",
    "        f_value = self.sigma(X,W)\n",
    "        K = self.K \n",
    "        loss_vector = np.zeros(X.shape[0])\n",
    "        for k in range(K):\n",
    "            loss_vector += np.log(f_value+1e-10)[:,k] * (y == k) # avoid nan issues\n",
    "        return -np.mean(loss_vector)\n",
    "                          \n",
    "    def loss_gradient(self,W,X,y):\n",
    "        f_value = self.sigma(X,W)\n",
    "        K = self.K \n",
    "        dLdW = np.zeros((X.shape[1],K))\n",
    "        for k in range(K):\n",
    "            dLdWk =(f_value[:,k] - (y==k)).reshape(-1,1)*X # Numpy broadcasting\n",
    "            dLdW[:,k] = np.mean(dLdWk, axis=0)   # RHS is 1D Numpy array -- so you can safely put it in the k-th column of 2D array dLdW\n",
    "        return dLdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X,y = load_digits(return_X_y = True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1     2     3     4     5    6    7    8    9   ...   54   55  \\\n",
       "0     0.0  0.0   5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1     0.0  0.0   0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2     0.0  0.0   0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  5.0  0.0   \n",
       "3     0.0  0.0   7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  9.0  0.0   \n",
       "4     0.0  0.0   0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...   ...  ...   ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1792  0.0  0.0   4.0  10.0  13.0   6.0  0.0  0.0  0.0  1.0  ...  4.0  0.0   \n",
       "1793  0.0  0.0   6.0  16.0  13.0  11.0  1.0  0.0  0.0  0.0  ...  1.0  0.0   \n",
       "1794  0.0  0.0   1.0  11.0  15.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1795  0.0  0.0   2.0  10.0   7.0   0.0  0.0  0.0  0.0  0.0  ...  2.0  0.0   \n",
       "1796  0.0  0.0  10.0  14.0   8.0   1.0  0.0  0.0  0.0  2.0  ...  8.0  0.0   \n",
       "\n",
       "       56   57   58    59    60    61   62   63  \n",
       "0     0.0  0.0  6.0  13.0  10.0   0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  11.0  16.0  10.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0   3.0  11.0  16.0  9.0  0.0  \n",
       "3     0.0  0.0  7.0  13.0  13.0   9.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0   2.0  16.0   4.0  0.0  0.0  \n",
       "...   ...  ...  ...   ...   ...   ...  ...  ...  \n",
       "1792  0.0  0.0  2.0  14.0  15.0   9.0  0.0  0.0  \n",
       "1793  0.0  0.0  6.0  16.0  14.0   6.0  0.0  0.0  \n",
       "1794  0.0  0.0  2.0   9.0  13.0   6.0  0.0  0.0  \n",
       "1795  0.0  0.0  5.0  12.0  16.0  12.0  0.0  0.0  \n",
       "1796  0.0  1.0  8.0  12.0  14.0  12.0  1.0  0.0  \n",
       "\n",
       "[1797 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(X)\n",
    "df #rows are samples of drawings, columns give pixels of the 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1617, 64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 iterations is:  2.2975031101988965\n",
      "loss after 501 iterations is:  0.9747646840265886\n",
      "loss after 1001 iterations is:  0.6271544957404386\n",
      "loss after 1501 iterations is:  0.48465074291917476\n",
      "loss after 2001 iterations is:  0.4067886795416971\n",
      "loss after 2501 iterations is:  0.3569853787369549\n",
      "loss after 3001 iterations is:  0.3219498860091718\n",
      "loss after 3501 iterations is:  0.2957112499207807\n",
      "loss after 4001 iterations is:  0.27517638606506345\n",
      "loss after 4501 iterations is:  0.2585728459578632\n",
      "loss after 5001 iterations is:  0.24480630370680928\n",
      "loss after 5501 iterations is:  0.23316150090969132\n",
      "loss after 6001 iterations is:  0.22314954388974834\n",
      "loss after 6501 iterations is:  0.21442400929215735\n",
      "loss after 7001 iterations is:  0.2067320488693829\n",
      "loss after 7501 iterations is:  0.19988447822601135\n",
      "loss after 8001 iterations is:  0.19373672640885967\n",
      "loss after 8501 iterations is:  0.18817628341982656\n",
      "loss after 9001 iterations is:  0.1831141858457873\n",
      "loss after 9501 iterations is:  0.17847909502105727\n",
      "loss after 10001 iterations is:  0.17421308722718495\n",
      "loss after 10501 iterations is:  0.17026860268039193\n",
      "loss after 11001 iterations is:  0.16660619607205016\n",
      "loss after 11501 iterations is:  0.16319285237565426\n",
      "loss after 12001 iterations is:  0.16000070825015078\n",
      "loss after 12501 iterations is:  0.15700606905300005\n",
      "loss after 13001 iterations is:  0.15418864437799004\n",
      "loss after 13501 iterations is:  0.15153094723746474\n",
      "loss after 14001 iterations is:  0.14901781725362154\n",
      "loss after 14501 iterations is:  0.14663603885543683\n",
      "loss after 15001 iterations is:  0.14437403299967985\n",
      "loss after 15501 iterations is:  0.1422216063268606\n",
      "loss after 16001 iterations is:  0.14016974557619977\n",
      "loss after 16501 iterations is:  0.13821044795587994\n",
      "loss after 17001 iterations is:  0.1363365802952386\n",
      "loss after 17501 iterations is:  0.1345417614013797\n",
      "loss after 18001 iterations is:  0.13282026324911267\n",
      "loss after 18501 iterations is:  0.13116692755309206\n",
      "loss after 19001 iterations is:  0.12957709497826864\n",
      "loss after 19501 iterations is:  0.12804654479263705\n"
     ]
    }
   ],
   "source": [
    "lg = myLogisticRegression(learning_rate=1e-4)\n",
    "lg.fit(X_train,y_train,n_iterations = 20000) # what about change the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lg.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coeff.shape #65 coefficients, 1 per row of data plus the constant term. 10 functions, 1 per class -- digits we are classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5,  71, 133, 149, 159], dtype=int64),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lg.predict(X_test)!=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2378ef17f10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAALEUlEQVR4nO3d649cdR3H8c/H7c0WSgURSbehkEATNJGSTQlpILEVUy4BjCa2CRgaTB9B2mBCwCfGfwDxARKaUiShQrRQQwiCDReBqJXeVMq2pjZIlwItAdxStKXl64OdmoKLe2b23Pbb9ytp2N2Z7O87wLtn9uzM+TkiBCCPzzU9AIByETWQDFEDyRA1kAxRA8lMquKbTvHUmKYZVXzrRn08q97HdGxqfWtNOu2j2tY6a8pwbWu9ceCM2taSpMlvH6plnX/rkI7EYY92WyVRT9MMXeLFVXzrRn246JJa1xue21fbWqdf9UZta62c+0xta/3oZ9+rbS1J+vJPfl/LOpvis/8d8vQbSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkimUNS2l9jeZXu37TuqHgpA78aM2nafpHskXSnpQknLbF9Y9WAAelPkSL1A0u6I2BMRRyQ9Ium6ascC0KsiUc+WtPeEz4c6X/sE2ytsb7a9+SMdLms+AF0qEvVob+/6n6sVRsTqiBiIiIHJqvE9gwA+oUjUQ5LmnPB5v6R91YwDYLyKRP2ypPNtn2t7iqSlkh6vdiwAvRrzIgkRcdT2LZKeltQnaW1E7Kh8MgA9KXTlk4h4UtKTFc8CoAS8ogxIhqiBZIgaSIaogWSIGkiGqIFkiBpIppIdOur04bfq2zXjxXvuq22tui1//bLa1rp+xge1rXXnabUt1RocqYFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSKbIDh1rbe+3/UodAwEYnyJH6p9LWlLxHABKMmbUEfGCpHdrmAVACUp7l5btFZJWSNI0TS/r2wLoUmknyth2B2gHzn4DyRA1kEyRX2k9LOkPkubZHrJ9c/VjAehVkb20ltUxCIBy8PQbSIaogWSIGkiGqIFkiBpIhqiBZIgaSGbCb7szPLev6REqc9UV361vsXfeq2+tbfUt1f/cv+pbrCU4UgPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEyRa5TNsf2c7UHbO2yvrGMwAL0p8trvo5J+EBFbbZ8qaYvtjRHxasWzAehBkW133oyIrZ2PD0oalDS76sEA9Kard2nZnitpvqRNo9zGtjtACxQ+UWb7FEmPSloVEcOfvp1td4B2KBS17ckaCXpdRDxW7UgAxqPI2W9Lul/SYETcVf1IAMajyJF6oaQbJS2yvb3z56qK5wLQoyLb7rwkyTXMAqAEvKIMSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWQm/F5aU9+LpkeozLEdu2pb672bLq1trV8fOqW2tT73uxo37moJjtRAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJFLjw4zfafbP+5s+3Oj+sYDEBvirxM9LCkRRHxQedSwS/Z/k1E/LHi2QD0oMiFB0PSB51PJ3f+5H3BNTDBFb2Yf5/t7ZL2S9oYEaNuu2N7s+3NH+lwyWMCKKpQ1BFxLCIuktQvaYHtr45yH7bdAVqgq7PfEfG+pOclLaliGADjV+Ts95m2Z3U+/rykb0jaWfFcAHpU5Oz32ZIetN2nkb8EfhkRT1Q7FoBeFTn7/ReN7EkNYALgFWVAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJOORd1aWa6ZPj0u8uPTv27S+r8yrdb06t905svGc2tbaP1zftjv9395R21p12hTPaDje9Wi3caQGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiCZwlF3Lui/zTYXHQRarJsj9UpJg1UNAqAcRbfd6Zd0taQ11Y4DYLyKHqnvlnS7pI8/6w7spQW0Q5EdOq6RtD8itvy/+7GXFtAORY7UCyVda/s1SY9IWmT7oUqnAtCzMaOOiDsjoj8i5kpaKunZiLih8skA9ITfUwPJFNkg778i4nmNbGULoKU4UgPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJdPV76pNdndvgSPVu83PfBWtrW+vmVbfVttbJiCM1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJFHqZaOdKogclHZN0NCIGqhwKQO+6ee331yPincomAVAKnn4DyRSNOiT91vYW2ytGuwPb7gDtUPTp98KI2Gf7S5I22t4ZES+ceIeIWC1ptSTN9OlR8pwACip0pI6IfZ1/7pe0QdKCKocC0LsiG+TNsH3q8Y8lfVPSK1UPBqA3RZ5+nyVpg+3j9/9FRDxV6VQAejZm1BGxR9LXapgFQAn4lRaQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDNvutNiu73+htrUumDyjtrWmb9hU21onI47UQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kUyhq27Nsr7e90/ag7UurHgxAb4q+9vunkp6KiO/YniJpeoUzARiHMaO2PVPS5ZJukqSIOCLpSLVjAehVkaff50k6IOkB29tsr+lc//sT2HYHaIciUU+SdLGkeyNivqRDku749J0iYnVEDETEwGRNLXlMAEUViXpI0lBEHH8T7HqNRA6ghcaMOiLekrTX9rzOlxZLerXSqQD0rOjZ71slreuc+d4jaXl1IwEYj0JRR8R2SQPVjgKgDLyiDEiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFk2Eurxab8s76/c5e/fllta0kHa1zr5MORGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIZsyobc+zvf2EP8O2V9UwG4AejPky0YjYJekiSbLdJ+kNSRuqHQtAr7p9+r1Y0t8j4h9VDANg/Lp9Q8dSSQ+PdoPtFZJWSNI09s8DGlP4SN255ve1kn412u1suwO0QzdPv6+UtDUi3q5qGADj103Uy/QZT70BtEehqG1Pl3SFpMeqHQfAeBXddudDSWdUPAuAEvCKMiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaScUSU/03tA5K6fXvmFyW9U/ow7ZD1sfG4mnNORJw52g2VRN0L25sjYqDpOaqQ9bHxuNqJp99AMkQNJNOmqFc3PUCFsj42HlcLteZnagDlaNORGkAJiBpIphVR215ie5ft3bbvaHqeMtieY/s524O2d9he2fRMZbLdZ3ub7SeanqVMtmfZXm97Z+e/3aVNz9Stxn+m7mwQ8DeNXC5pSNLLkpZFxKuNDjZOts+WdHZEbLV9qqQtkq6f6I/rONu3SRqQNDMirml6nrLYflDSixGxpnMF3ekR8X7DY3WlDUfqBZJ2R8SeiDgi6RFJ1zU807hFxJsRsbXz8UFJg5JmNztVOWz3S7pa0pqmZymT7ZmSLpd0vyRFxJGJFrTUjqhnS9p7wudDSvI//3G250qaL2lTw6OU5W5Jt0v6uOE5ynaepAOSHuj8aLHG9oymh+pWG6L2KF9L83s226dIelTSqogYbnqe8bJ9jaT9EbGl6VkqMEnSxZLujYj5kg5JmnDneNoQ9ZCkOSd83i9pX0OzlMr2ZI0EvS4islxeeaGka22/ppEflRbZfqjZkUozJGkoIo4/o1qvkcgnlDZE/bKk822f2zkxsVTS4w3PNG62rZGfzQYj4q6m5ylLRNwZEf0RMVcj/62ejYgbGh6rFBHxlqS9tud1vrRY0oQ7sdntBnmli4ijtm+R9LSkPklrI2JHw2OVYaGkGyX91fb2ztd+GBFPNjcSCrhV0rrOAWaPpOUNz9O1xn+lBaBcbXj6DaBERA0kQ9RAMkQNJEPUQDJEDSRD1EAy/wF6yp/HH9s1KgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_test[133,].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 7\n"
     ]
    }
   ],
   "source": [
    "print(lg.predict(X_test)[133],y_test[133])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) can provide as more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 10,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 17,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 16,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 25,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 21,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0, 19,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 18,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  8,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1, 24]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,lg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First row**: seventeen 0s were classified as 0\n",
    "\n",
    "**Second row**: ten 1s were classified as 1, one 1 was classified as 2\n",
    "\n",
    "**Exercise**: Determine what the rest of the rows tell us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricks in training: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're doing the final project, it's very likely that you might lose patience -- training on the 60,000 MNIST data is VERY SLOW! (of course it's not an excuse to abandon the project lol)\n",
    "\n",
    "To speed up the training process (most importantly the optimization algorithm), there are two directions of general strategies:\n",
    "    - find better algorithm whose convergence is faster (you take less steps to arrive at the minimum)\n",
    "    - save the computational cost within each step\n",
    "    \n",
    "Of course there are trade-offs between these two directions.\n",
    "\n",
    "    \n",
    "**Basic observation of SGD**: Calculating the gradient in each step is TOO EXPENSIVE! \n",
    "\n",
    "Recall that in general supervised learning, $$\\nabla_{\\beta} L(\\beta;X,Y) = \\frac{1}{N}\\sum_{i=1}^{N}\\nabla_{\\beta}l(\\beta;x^{(i)},y^{(i)})$$\n",
    "\n",
    "It means that we need to implement 60,000 sum calculation in the single step!!!\n",
    "\n",
    "**\"Wild\" yet smart idea**: Note that the RHS is in the form of \"population average\". The basic intuitive from statistics is that we can use \"sample means\" to replace \"population average\". If you're bold enough -- just randomly pick up ONE single sample and use this value to replace \"population average\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Herustic expression of \"pure stochastic\" SGD: $$\\beta^{k+1}=\\beta^{k}-\\eta \\nabla_{\\beta}l(\\beta^{k};x^{(r)},y^{(r)}),$$ where $r$ denotes the index randomly picked during this step.\n",
    "\n",
    "    \n",
    "- (mini-batch SGD, or \"standard\" SGD):$$\\beta^{k+1}=\\beta^{k}-\\eta \\frac{1}{n_{B}}\\sum_{k=1}^{n_{B}}\\nabla_{\\beta}l(\\beta^{k};x^{(k)},y^{(k)}),$$ where $n_b$ denotes the size of mini-batch, and the average is taken over the $n_b$ random samples.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In actual programming, we don't want to generate new random numbers in each step, nor want to \"waste\" some samples -- we desire all training data can be used during SGD. It is very useful to adopt the \"epoch-batch\" strategy (or called cyclic rule) through permutation of the data.\n",
    "\n",
    "> Choose initial guess $\\beta^{0}$, step size (learning rate) $\\eta$, <br>\n",
    "batch size $n_B$, number of inner iterations $M\\leq N/n_B$, number of epochs $n_E$ <br><br>\n",
    ">    For epoch $n=1,2, \\cdots, n_E$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\beta^{0}$ for the current epoch is $\\beta^{M+1}$ for the previous epoch.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the training samples.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M-1$ <br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\beta^{m+1} = {\\beta}^m -  \\frac{\\eta}{n_B}\\sum_{i=1}^{n_B} \\nabla_{\\beta} l(\\beta^{m}; \n",
    "x^{(m*n_{B}+i)},y^{(m*n_{B}+i)})$\n",
    "\n",
    "If the gradient loss of your program is written in a highly vectorized way (it supports a data matrix as the input), then you can simply make the data matrix within the mini-batch as the input in each GD update. Below is the example based on our previous binary logistic regression codes. \n",
    "\n",
    "In practice, you may also find it helpful to adjust the stepsize (learning rate $\\eta$) during the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogisticRegression_binary():\n",
    "    \"\"\" Logistic Regression classifier -- this only works for the binary case. Here we provide the option of SGD in optimization.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.001, opt_method = 'SGD', num_epochs = 50, size_batch = 20):\n",
    "        \n",
    "        # learning rate can also be in the fit method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.opt_method = opt_method\n",
    "        self.num_epochs = num_epochs\n",
    "        self.size_batch = size_batch\n",
    "        \n",
    "        \n",
    "    def fit(self, data, y, n_iterations = 1000):\n",
    "        \"\"\" \n",
    "        don't forget the document string in methods\n",
    "        \"\"\"\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        beta  = np.zeros(np.shape(X)[1]) # initialize beta, can be other choices\n",
    "\n",
    "        if self.opt_method == 'GD':\n",
    "            for k in range(n_iterations):\n",
    "                dbeta = self.loss_gradient(beta,X,y) # write another function to compute gradient\n",
    "                beta = beta - eta * dbeta # the formula of GD\n",
    "                # this step is optional -- just for inspection purposes\n",
    "                if k % 500 == 0: # pprint loss every 500 steps\n",
    "                    print(\"loss after\", k+1, \"iterations is: \", self.loss(beta,X,y))\n",
    "        \n",
    "        if self.opt_method == 'SGD':\n",
    "            N = X.shape[0]\n",
    "            num_epochs = self.num_epochs\n",
    "            size_batch = self.size_batch\n",
    "            num_iter = 0\n",
    "            for e in range(num_epochs):\n",
    "                shuffle_index = np.random.permutation(N) # in each epoch, we first reshuffle the data to create \"randomness\"\n",
    "                for m in range(0,N,size_batch):   # m is the starting index of mini-batch\n",
    "                    i = shuffle_index[m:m+size_batch] # index of samples in the mini-batch\n",
    "                    dbeta = self.loss_gradient(beta,X[i,:],y[i]) # only use the data in mini-batch to compute gradient. Note the average is taken in the loss_gradient function\n",
    "                    beta = beta - eta * dbeta # the formula of GD, but this time dbeta is different\n",
    "                \n",
    "                    if e % 1 == 0 and num_iter % 500 ==0: # print loss during the training process\n",
    "                        print(\"loss after\", e+1, \"epochs and \", num_iter+1, \"iterations is: \", self.loss(beta,X,y))\n",
    "        \n",
    "                    num_iter = num_iter +1  # number of total iterations\n",
    "            \n",
    "        self.coeff = beta\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        beta = self.coeff # the estimated beta\n",
    "        y_pred = np.round(self.sigmoid(np.dot(X,beta))).astype(int) # >0.5: ->1 else,->0 -- note that we always use Numpy universal functions when possible\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, data, y_true):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        y_pred = self.predict(data)\n",
    "        acc = np.mean(y_pred == y_true) # number of correct predictions/N\n",
    "        return acc\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def loss(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))\n",
    "        loss_value = np.log(f_value + 1e-10) * y + (1.0 - y)* np.log(1 - f_value + 1e-10) # avoid nan issues\n",
    "        return -np.mean(loss_value)\n",
    "                          \n",
    "    def loss_gradient(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))                  \n",
    "        gradient_value = (f_value - y).reshape(-1,1)*X # this is the hardest expression -- check yourself\n",
    "        return np.mean(gradient_value, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find adapting the SGD codes above to multi-class logistic regression is very helpful in doing your final project! (although it's not basic requirement). Here is the very intuitive argument when SGD can boost the algorithms.\n",
    "\n",
    "Suppose in the training dataset you have $N= 60,000$ samples. With GD, each iteration will cost 60,000 summations. Now consider using SGD. We have the mini-batch size of 30. Then each iteration will cost only 30 sums. For a complete epoch, you have 60,000 sums -- the same with GD, but you have already iterated for 2000 steps!\n",
    "\n",
    "Of course you may argue that the \"quality\" of steps in GD is \"far better\" than SGD. Surely there is the trade-off, but pratically [the inferior performace of SGD in convergence does not obscure its super efficiency over GD](https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/stochastic-gd.pdf). In fact, SGD is the de facto optimization method in deep learning. (SGD and BP -- backward propogation to calculate the gradient are the two fundamental cornerstones in deep learning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compare GD and SGD with the UCI [\"adult\" dataset](https://archive.ics.uci.edu/ml/datasets/adult) to predict income. Note that it is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt     education  educational-num  \\\n",
       "0       25       Private  226802          11th                7   \n",
       "1       38       Private   89814       HS-grad                9   \n",
       "2       28     Local-gov  336951    Assoc-acdm               12   \n",
       "3       44       Private  160323  Some-college               10   \n",
       "4       18             ?  103497  Some-college               10   \n",
       "...    ...           ...     ...           ...              ...   \n",
       "48837   27       Private  257302    Assoc-acdm               12   \n",
       "48838   40       Private  154374       HS-grad                9   \n",
       "48839   58       Private  151910       HS-grad                9   \n",
       "48840   22       Private  201490       HS-grad                9   \n",
       "48841   52  Self-emp-inc  287927       HS-grad                9   \n",
       "\n",
       "           marital-status         occupation relationship   race  gender  \\\n",
       "0           Never-married  Machine-op-inspct    Own-child  Black    Male   \n",
       "1      Married-civ-spouse    Farming-fishing      Husband  White    Male   \n",
       "2      Married-civ-spouse    Protective-serv      Husband  White    Male   \n",
       "3      Married-civ-spouse  Machine-op-inspct      Husband  Black    Male   \n",
       "4           Never-married                  ?    Own-child  White  Female   \n",
       "...                   ...                ...          ...    ...     ...   \n",
       "48837  Married-civ-spouse       Tech-support         Wife  White  Female   \n",
       "48838  Married-civ-spouse  Machine-op-inspct      Husband  White    Male   \n",
       "48839             Widowed       Adm-clerical    Unmarried  White  Female   \n",
       "48840       Never-married       Adm-clerical    Own-child  White    Male   \n",
       "48841  Married-civ-spouse    Exec-managerial         Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0                 0             0              40  United-States  <=50K  \n",
       "1                 0             0              50  United-States  <=50K  \n",
       "2                 0             0              40  United-States   >50K  \n",
       "3              7688             0              40  United-States   >50K  \n",
       "4                 0             0              30  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "48837             0             0              38  United-States  <=50K  \n",
       "48838             0             0              40  United-States   >50K  \n",
       "48839             0             0              40  United-States  <=50K  \n",
       "48840             0             0              20  United-States  <=50K  \n",
       "48841         15024             0              40  United-States   >50K  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('adult.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18        NaN  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                NaN    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import nan\n",
    "df = df.replace('?',nan) #dealing with missing values -- ? in original dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>198693</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt     education  educational-num  \\\n",
       "0       25       Private  226802          11th                7   \n",
       "1       38       Private   89814       HS-grad                9   \n",
       "2       28     Local-gov  336951    Assoc-acdm               12   \n",
       "3       44       Private  160323  Some-college               10   \n",
       "5       34       Private  198693          10th                6   \n",
       "...    ...           ...     ...           ...              ...   \n",
       "48837   27       Private  257302    Assoc-acdm               12   \n",
       "48838   40       Private  154374       HS-grad                9   \n",
       "48839   58       Private  151910       HS-grad                9   \n",
       "48840   22       Private  201490       HS-grad                9   \n",
       "48841   52  Self-emp-inc  287927       HS-grad                9   \n",
       "\n",
       "           marital-status         occupation   relationship   race  gender  \\\n",
       "0           Never-married  Machine-op-inspct      Own-child  Black    Male   \n",
       "1      Married-civ-spouse    Farming-fishing        Husband  White    Male   \n",
       "2      Married-civ-spouse    Protective-serv        Husband  White    Male   \n",
       "3      Married-civ-spouse  Machine-op-inspct        Husband  Black    Male   \n",
       "5           Never-married      Other-service  Not-in-family  White    Male   \n",
       "...                   ...                ...            ...    ...     ...   \n",
       "48837  Married-civ-spouse       Tech-support           Wife  White  Female   \n",
       "48838  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
       "48839             Widowed       Adm-clerical      Unmarried  White  Female   \n",
       "48840       Never-married       Adm-clerical      Own-child  White    Male   \n",
       "48841  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0                 0             0              40  United-States  <=50K  \n",
       "1                 0             0              50  United-States  <=50K  \n",
       "2                 0             0              40  United-States   >50K  \n",
       "3              7688             0              40  United-States   >50K  \n",
       "5                 0             0              30  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "48837             0             0              38  United-States  <=50K  \n",
       "48838             0             0              40  United-States   >50K  \n",
       "48839             0             0              40  United-States  <=50K  \n",
       "48840             0             0              20  United-States  <=50K  \n",
       "48841         15024             0              40  United-States   >50K  \n",
       "\n",
       "[45222 rows x 15 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace = True) # drop missing values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass     education  educational-num      marital-status  \\\n",
       "0       25       Private          11th                7       Never-married   \n",
       "1       38       Private       HS-grad                9  Married-civ-spouse   \n",
       "2       28     Local-gov    Assoc-acdm               12  Married-civ-spouse   \n",
       "3       44       Private  Some-college               10  Married-civ-spouse   \n",
       "5       34       Private          10th                6       Never-married   \n",
       "...    ...           ...           ...              ...                 ...   \n",
       "48837   27       Private    Assoc-acdm               12  Married-civ-spouse   \n",
       "48838   40       Private       HS-grad                9  Married-civ-spouse   \n",
       "48839   58       Private       HS-grad                9             Widowed   \n",
       "48840   22       Private       HS-grad                9       Never-married   \n",
       "48841   52  Self-emp-inc       HS-grad                9  Married-civ-spouse   \n",
       "\n",
       "              occupation   relationship   race  gender  capital-gain  \\\n",
       "0      Machine-op-inspct      Own-child  Black    Male             0   \n",
       "1        Farming-fishing        Husband  White    Male             0   \n",
       "2        Protective-serv        Husband  White    Male             0   \n",
       "3      Machine-op-inspct        Husband  Black    Male          7688   \n",
       "5          Other-service  Not-in-family  White    Male             0   \n",
       "...                  ...            ...    ...     ...           ...   \n",
       "48837       Tech-support           Wife  White  Female             0   \n",
       "48838  Machine-op-inspct        Husband  White    Male             0   \n",
       "48839       Adm-clerical      Unmarried  White  Female             0   \n",
       "48840       Adm-clerical      Own-child  White    Male             0   \n",
       "48841    Exec-managerial           Wife  White  Female         15024   \n",
       "\n",
       "       capital-loss  hours-per-week income  \n",
       "0                 0              40  <=50K  \n",
       "1                 0              50  <=50K  \n",
       "2                 0              40   >50K  \n",
       "3                 0              40   >50K  \n",
       "5                 0              30  <=50K  \n",
       "...             ...             ...    ...  \n",
       "48837             0              38  <=50K  \n",
       "48838             0              40   >50K  \n",
       "48839             0              40  <=50K  \n",
       "48840             0              20  <=50K  \n",
       "48841             0              40   >50K  \n",
       "\n",
       "[45222 rows x 13 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['fnlwgt','native-country'], inplace=True) # drop some variables we are not interested\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  workclass  education  educational-num  marital-status  occupation  \\\n",
       "0        8          2          1                6               4           6   \n",
       "1       21          2         11                8               2           4   \n",
       "2       11          1          7               11               2          10   \n",
       "3       27          2         15                9               2           6   \n",
       "5       17          2          0                5               4           7   \n",
       "...    ...        ...        ...              ...             ...         ...   \n",
       "48837   10          2          7               11               2          12   \n",
       "48838   23          2         11                8               2           6   \n",
       "48839   41          2         11                8               6           0   \n",
       "48840    5          2         11                8               4           0   \n",
       "48841   35          3         11                8               2           3   \n",
       "\n",
       "       relationship  race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                 3     2       1             0             0              39   \n",
       "1                 0     4       1             0             0              49   \n",
       "2                 0     4       1             0             0              39   \n",
       "3                 0     2       1            96             0              39   \n",
       "5                 1     4       1             0             0              29   \n",
       "...             ...   ...     ...           ...           ...             ...   \n",
       "48837             5     4       0             0             0              37   \n",
       "48838             0     4       1             0             0              39   \n",
       "48839             4     4       0             0             0              39   \n",
       "48840             3     4       1             0             0              19   \n",
       "48841             5     4       0           110             0              39   \n",
       "\n",
       "       income  \n",
       "0           0  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "5           0  \n",
       "...       ...  \n",
       "48837       0  \n",
       "48838       1  \n",
       "48839       0  \n",
       "48840       0  \n",
       "48841       1  \n",
       "\n",
       "[45222 rows x 13 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_clean = df.apply(LabelEncoder().fit_transform) # transform the categorical variables into numerical\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is not best way to encode the data. Please see other solutions in [kaggle](https://www.kaggle.com/wenruliu/adult-income-dataset/notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean['income'].to_numpy()\n",
    "X = df_clean.drop(columns = 'income').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45222, 12)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukea\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8273269953570639"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_gd = myLogisticRegression_binary(learning_rate=1e-6, opt_method = 'GD')\n",
    "lg_sgd = myLogisticRegression_binary(learning_rate=1e-6, opt_method = 'SGD', num_epochs = 15, size_batch = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 iterations is:  0.6930358550277247\n",
      "loss after 501 iterations is:  0.6503339171382144\n",
      "loss after 1001 iterations is:  0.6250322404153786\n",
      "loss after 1501 iterations is:  0.6091127195017652\n",
      "loss after 2001 iterations is:  0.5984037857262677\n",
      "loss after 2501 iterations is:  0.590712724359857\n",
      "loss after 3001 iterations is:  0.5848586907302408\n",
      "loss after 3501 iterations is:  0.5801861202580018\n",
      "loss after 4001 iterations is:  0.5763181444418489\n",
      "loss after 4501 iterations is:  0.5730292982409765\n",
      "loss after 5001 iterations is:  0.570178434214311\n",
      "loss after 5501 iterations is:  0.567672659406349\n",
      "loss after 6001 iterations is:  0.565447582899603\n",
      "loss after 6501 iterations is:  0.5634562915787977\n",
      "loss after 7001 iterations is:  0.5616630677823014\n",
      "loss after 7501 iterations is:  0.5600397185713463\n",
      "loss after 8001 iterations is:  0.5585633633136624\n",
      "loss after 8501 iterations is:  0.5572150485499265\n",
      "loss after 9001 iterations is:  0.555978841583727\n",
      "loss after 9501 iterations is:  0.5548412083490464\n",
      "loss after 10001 iterations is:  0.5537905657746116\n",
      "loss after 10501 iterations is:  0.5528169456723574\n",
      "loss after 11001 iterations is:  0.551911733237817\n",
      "loss after 11501 iterations is:  0.5510674578925445\n",
      "loss after 12001 iterations is:  0.5502776225312458\n",
      "loss after 12501 iterations is:  0.5495365620648746\n",
      "loss after 13001 iterations is:  0.5488393250200673\n",
      "loss after 13501 iterations is:  0.548181573717374\n",
      "loss after 14001 iterations is:  0.5475594996778428\n",
      "loss after 14501 iterations is:  0.5469697516624197\n",
      "CPU times: total: 1min 11s\n",
      "Wall time: 57.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg_gd.fit(X_train,y_train,n_iterations = 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7950475348220207"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_gd.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 epochs and  1 iterations is:  0.6930377397465836\n",
      "loss after 1 epochs and  501 iterations is:  0.6500590971685041\n",
      "loss after 1 epochs and  1001 iterations is:  0.6249660635953507\n",
      "loss after 2 epochs and  1501 iterations is:  0.6091068807664944\n",
      "loss after 2 epochs and  2001 iterations is:  0.5984046743702526\n",
      "loss after 3 epochs and  2501 iterations is:  0.590848944811094\n",
      "loss after 3 epochs and  3001 iterations is:  0.5848485300498646\n",
      "loss after 4 epochs and  3501 iterations is:  0.5802138309488267\n",
      "loss after 4 epochs and  4001 iterations is:  0.5762701835277015\n",
      "loss after 5 epochs and  4501 iterations is:  0.5729892188339624\n",
      "loss after 5 epochs and  5001 iterations is:  0.5702264180799361\n",
      "loss after 6 epochs and  5501 iterations is:  0.5676835881914284\n",
      "loss after 6 epochs and  6001 iterations is:  0.5654714176508571\n",
      "loss after 7 epochs and  6501 iterations is:  0.5635054838849006\n",
      "loss after 7 epochs and  7001 iterations is:  0.5616737682333379\n",
      "loss after 8 epochs and  7501 iterations is:  0.5600356544144481\n",
      "loss after 8 epochs and  8001 iterations is:  0.5585409711895406\n",
      "loss after 9 epochs and  8501 iterations is:  0.5571917362220928\n",
      "loss after 9 epochs and  9001 iterations is:  0.5559931371012239\n",
      "loss after 10 epochs and  9501 iterations is:  0.5548449965718294\n",
      "loss after 10 epochs and  10001 iterations is:  0.5537796589805646\n",
      "loss after 11 epochs and  10501 iterations is:  0.5528557334684926\n",
      "loss after 11 epochs and  11001 iterations is:  0.5519388734455605\n",
      "loss after 12 epochs and  11501 iterations is:  0.5510832730673528\n",
      "loss after 12 epochs and  12001 iterations is:  0.5502753381051326\n",
      "loss after 13 epochs and  12501 iterations is:  0.5495646814084445\n",
      "loss after 13 epochs and  13001 iterations is:  0.548849883700451\n",
      "loss after 14 epochs and  13501 iterations is:  0.548172204938173\n",
      "loss after 14 epochs and  14001 iterations is:  0.5475386104422478\n",
      "loss after 15 epochs and  14501 iterations is:  0.5469668989431431\n",
      "loss after 15 epochs and  15001 iterations is:  0.5464310676408828\n",
      "CPU times: total: 688 ms\n",
      "Wall time: 569 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg_sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7950475348220207"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_sgd.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Reading Suggestions \n",
    "\n",
    "- ISLR: Chapter 4\n",
    "- ESL: Chapter 4\n",
    "- PML: Chapter 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
